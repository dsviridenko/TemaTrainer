{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10486910,"sourceType":"datasetVersion","datasetId":6493015},{"sourceId":10487199,"sourceType":"datasetVersion","datasetId":6493189},{"sourceId":10507302,"sourceType":"datasetVersion","datasetId":6504914},{"sourceId":10508719,"sourceType":"datasetVersion","datasetId":6505935},{"sourceId":10515037,"sourceType":"datasetVersion","datasetId":6508590},{"sourceId":10515842,"sourceType":"datasetVersion","datasetId":6509059},{"sourceId":10517801,"sourceType":"datasetVersion","datasetId":6510253},{"sourceId":10518556,"sourceType":"datasetVersion","datasetId":6510691},{"sourceId":10529481,"sourceType":"datasetVersion","datasetId":6516296},{"sourceId":10535528,"sourceType":"datasetVersion","datasetId":6518777},{"sourceId":10627320,"sourceType":"datasetVersion","datasetId":6579940},{"sourceId":12703805,"sourceType":"datasetVersion","datasetId":8028844},{"sourceId":12703807,"sourceType":"datasetVersion","datasetId":8028846},{"sourceId":12704083,"sourceType":"datasetVersion","datasetId":8029047},{"sourceId":12710359,"sourceType":"datasetVersion","datasetId":8033262},{"sourceId":12718022,"sourceType":"datasetVersion","datasetId":8038367},{"sourceId":12718031,"sourceType":"datasetVersion","datasetId":8038375},{"sourceId":12718273,"sourceType":"datasetVersion","datasetId":8038558},{"sourceId":12720094,"sourceType":"datasetVersion","datasetId":8039799},{"sourceId":12720324,"sourceType":"datasetVersion","datasetId":8039963},{"sourceId":12724565,"sourceType":"datasetVersion","datasetId":8042702},{"sourceId":12724568,"sourceType":"datasetVersion","datasetId":8042704},{"sourceId":12724879,"sourceType":"datasetVersion","datasetId":8042932},{"sourceId":12727429,"sourceType":"datasetVersion","datasetId":8044632},{"sourceId":12727433,"sourceType":"datasetVersion","datasetId":8044633},{"sourceId":12727811,"sourceType":"datasetVersion","datasetId":8044889},{"sourceId":12746290,"sourceType":"datasetVersion","datasetId":8057594},{"sourceId":12746697,"sourceType":"datasetVersion","datasetId":8057868},{"sourceId":12746847,"sourceType":"datasetVersion","datasetId":8057960},{"sourceId":12750775,"sourceType":"datasetVersion","datasetId":8060356},{"sourceId":12751813,"sourceType":"datasetVersion","datasetId":8061117},{"sourceId":12752579,"sourceType":"datasetVersion","datasetId":8061634},{"sourceId":12760957,"sourceType":"datasetVersion","datasetId":8066856},{"sourceId":12760967,"sourceType":"datasetVersion","datasetId":8066863},{"sourceId":12760972,"sourceType":"datasetVersion","datasetId":8066868},{"sourceId":12761501,"sourceType":"datasetVersion","datasetId":8067250},{"sourceId":12763363,"sourceType":"datasetVersion","datasetId":8068523},{"sourceId":12763388,"sourceType":"datasetVersion","datasetId":8068540},{"sourceId":12763623,"sourceType":"datasetVersion","datasetId":8068700},{"sourceId":12763793,"sourceType":"datasetVersion","datasetId":8068814},{"sourceId":12771386,"sourceType":"datasetVersion","datasetId":8073787},{"sourceId":12771391,"sourceType":"datasetVersion","datasetId":8073790},{"sourceId":12921839,"sourceType":"datasetVersion","datasetId":8176484},{"sourceId":12922602,"sourceType":"datasetVersion","datasetId":8177035},{"sourceId":12930687,"sourceType":"datasetVersion","datasetId":8182452},{"sourceId":12930689,"sourceType":"datasetVersion","datasetId":8182454},{"sourceId":12931712,"sourceType":"datasetVersion","datasetId":8183140},{"sourceId":12931715,"sourceType":"datasetVersion","datasetId":8183143},{"sourceId":12939002,"sourceType":"datasetVersion","datasetId":8187885},{"sourceId":12939999,"sourceType":"datasetVersion","datasetId":8188583},{"sourceId":12940003,"sourceType":"datasetVersion","datasetId":8188587},{"sourceId":12940591,"sourceType":"datasetVersion","datasetId":8188979},{"sourceId":12947455,"sourceType":"datasetVersion","datasetId":8193558},{"sourceId":12947460,"sourceType":"datasetVersion","datasetId":8193562},{"sourceId":12948107,"sourceType":"datasetVersion","datasetId":8194022},{"sourceId":12948962,"sourceType":"datasetVersion","datasetId":8194609},{"sourceId":12948968,"sourceType":"datasetVersion","datasetId":8194614},{"sourceId":12949720,"sourceType":"datasetVersion","datasetId":8195158},{"sourceId":12949721,"sourceType":"datasetVersion","datasetId":8195159}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ИМПОРТЫ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade --no-cache-dir --no-deps unsloth, transformers accelerate peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:11:13.497976Z","iopub.execute_input":"2025-09-10T14:11:13.498738Z","iopub.status.idle":"2025-09-10T14:11:14.609851Z","shell.execute_reply.started":"2025-09-10T14:11:13.498702Z","shell.execute_reply":"2025-09-10T14:11:14.608819Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# ОБУЧЕНИЕ БАЗОВОЙ МОДЕЛИ","metadata":{}},{"cell_type":"markdown","source":"## Импортируем модель и вводим параметры","metadata":{}},{"cell_type":"code","source":"# import unsloth\nfrom unsloth import FastLanguageModel\nimport torch\n\n\nmax_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\noutput_dir = 'Qwen2-1.5B-bnb-4bit-kl-sft-base1' # Введите свое название модели после файнтюнинга\nmodel_name = \"unsloth/Qwen2-1.5B-bnb-4bit\" # Введите название модели\n# model_name = 'unsloth/gemma-2-2b-bnb-4bit'\n# model_name = 'unsloth/mistral-7b-v0.3-bnb-4bit'\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:11:19.009846Z","iopub.execute_input":"2025-09-10T14:11:19.010142Z","iopub.status.idle":"2025-09-10T14:11:19.032288Z","shell.execute_reply.started":"2025-09-10T14:11:19.010107Z","shell.execute_reply":"2025-09-10T14:11:19.031133Z"},"_kg_hide-input":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3279844005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import unsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Try loading bitsandbytes and triton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mcdequantize_blockwise_fp32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdequantize_blockwise_fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"],"ename":"ModuleNotFoundError","evalue":"No module named 'bitsandbytes'","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"## Добавляем параметры LoRa","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:02:24.883495Z","iopub.execute_input":"2025-09-09T14:02:24.883933Z","iopub.status.idle":"2025-09-09T14:02:31.852958Z","shell.execute_reply.started":"2025-09-09T14:02:24.883901Z","shell.execute_reply":"2025-09-09T14:02:31.852333Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.9.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Подготавливаем датасет","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_excel('/kaggle/input/dataf0/data 0.xlsx')\n\nprint(train_data[['description', 'target']].head())\n\ny = train_data['target']\nX_train = train_data[['description', 'target']]\n\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n\n\nalpaca_prompt = \"\"\"\n### Input:\n{}\n\n### Output:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    inputs = examples[\"description\"]\n    outputs = examples[\"target\"]\n    texts = []\n    for input, output in zip(inputs, outputs):\n        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\nfrom datasets import Dataset,DatasetDict\ntrain_dataset_dict = DatasetDict({\n    \"train\": Dataset.from_pandas(X_train),\n})\n\ntrain_dataset_dict = train_dataset_dict.map(formatting_prompts_func, batched = True)\ntrain_dataset_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:02:32.013821Z","iopub.execute_input":"2025-09-09T14:02:32.014111Z","iopub.status.idle":"2025-09-09T14:02:33.152452Z","shell.execute_reply.started":"2025-09-09T14:02:32.014087Z","shell.execute_reply":"2025-09-09T14:02:33.151596Z"}},"outputs":[{"name":"stdout","text":"                                         description  \\\n0  л14 материал представлен содержимым кистозно-г...   \n1  П 29 (№3683/20). На фоне \"жидкого\" коллоидаизм...   \n2  л22 в мазке  неравномерной толщины с обильным ...   \n3  пунктирован узел коллоидного в разной степени ...   \n4  П6 в мазке высокой клеточности фоне кистозно г...   \n\n                                              target  \n0  В соответствии с критериями   системы классифи...  \n1  Неинформативное исследование в соответствии с ...  \n2  В соответствии с критериями системы классифика...  \n3                            Bethesda - категория II  \n4                             Bethesda - категория V  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fa80438dce4ed7bbb34af33f8c60ef"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['description', 'target', 'text'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_dataset_dict['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:02:37.200831Z","iopub.execute_input":"2025-09-09T14:02:37.201110Z","iopub.status.idle":"2025-09-09T14:02:37.207193Z","shell.execute_reply.started":"2025-09-09T14:02:37.201089Z","shell.execute_reply":"2025-09-09T14:02:37.206364Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'description': 'л14 материал представлен содержимым кистозно-геморрагической полостиобильным плотным  коллоидом  на фоне которых обнаружены фрагменты лизированных клеток и  единичные мелкие бесформенные скопления  полиморфных   эпителиальных клеток с выраженными дегенеративными изменениями.',\n 'target': 'В соответствии с критериями   системы классификации Bethesdа: материанедостаточно информативный диагностическая категория I',\n 'text': '\\n### Input:\\nл14 материал представлен содержимым кистозно-геморрагической полостиобильным плотным  коллоидом  на фоне которых обнаружены фрагменты лизированных клеток и  единичные мелкие бесформенные скопления  полиморфных   эпителиальных клеток с выраженными дегенеративными изменениями.\\n\\n### Output:\\nВ соответствии с критериями   системы классификации Bethesdа: материанедостаточно информативный диагностическая категория I<|endoftext|>'}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Вводим параметры обучения","metadata":{}},{"cell_type":"code","source":"import torch\nfrom trl import SFTTrainer\nfrom transformers.trainer import Accelerator\nimport warnings\n\nclass CustomSFTTrainer_w_kl(SFTTrainer):\n    def __init__(self, kl_alpha=0.1, **kwargs):\n        \n        super().__init__(**kwargs)\n\n        if not hasattr(model, 'disable_adapter'):\n            warnings.warn(\n                \"The model provided does not appear to be a PEFT model with a LoRA adapter. \"\n                \"The KL divergence loss term will not be calculated and the trainer will fall back to standard SFT.\"\n            )\n\n        self.accelerator = Accelerator()\n        self._total_train_tokens = 0\n        # After super().__init__, self._metrics is initialized by the parent Trainer class.\n        # We need to add our custom metric to it.\n        if \"kl_loss\" not in self._metrics[\"train\"]:\n            self._metrics[\"train\"][\"kl_loss\"] = []\n        if \"eval\" in self._metrics and \"kl_loss\" not in self._metrics[\"eval\"]:\n            self._metrics[\"eval\"][\"kl_loss\"] = []\n        self.kl_alpha = kl_alpha\n\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Computes SFT loss and adds a KL divergence penalty term.\n        The reference log probabilities are computed on the fly by disabling the LoRA adapter.\n        \"\"\"\n        if self.model_accepts_loss_kwargs:\n            loss_kwargs = {}\n            if num_items_in_batch is not None:\n                loss_kwargs[\"num_items_in_batch\"] = num_items_in_batch\n        # Compute standard SFT loss with the adapter enabled\n        sft_loss, outputs = super().compute_loss(model, inputs, return_outputs=True, **loss_kwargs)\n\n        # If the model is not a PEFT model, we can't compute KL loss, so we return SFT loss\n        if not hasattr(model, 'disable_adapter'):\n            warnings.warn(\n                \"The model provided does not appear to be a PEFT model with a LoRA adapter. \"\n                \"The KL divergence loss term will not be calculated and the trainer will fall back to standard SFT.\"\n            )\n            return (sft_loss, outputs) if return_outputs else sft_loss\n\n        # Get the logits from the adapter-enabled model\n        logits = outputs.logits\n        labels = inputs[\"labels\"]\n\n        # Compute reference logits by disabling the adapter\n        with torch.no_grad(), model.disable_adapter():\n            ref_outputs = model(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs.get(\"attention_mask\", None),\n            )\n        ref_logits = ref_outputs.logits\n        \n        # print(ref_logits)\n        # Shift all tensors for next-token prediction\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        shift_ref_logits = ref_logits[..., :-1, :].contiguous()\n\n        # Create a mask for the answer tokens (non -100 labels)\n        answer_mask = (shift_labels != -100).float()\n\n        # Calculate log probabilities from the adapter model and reference model logits\n        log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n        ref_log_probs = torch.nn.functional.log_softmax(shift_ref_logits, dim=-1)\n\n        # Calculate KL divergence: KL(P_ref || P_model) = sum(P_ref * (log P_ref - log P_model))\n        kl_divergence = torch.exp(ref_log_probs) * (ref_log_probs - log_probs)\n\n        # Sum over the vocabulary dimension and apply the mask and average over the sequence and batch\n        kl_loss_per_token = (kl_divergence.sum(dim=-1) * answer_mask)\n        kl_loss = kl_loss_per_token.sum() / answer_mask.sum()\n\n        # Combine losses\n        loss = sft_loss + self.kl_alpha * kl_loss\n\n        # Logging\n        self._metrics[\"train\"][\"kl_loss\"].append(kl_loss.item())\n\n        return (loss, outputs) if return_outputs else loss\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = False,\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\n# Инициализация и запуск\ntrainer = CustomSFTTrainer_w_kl(\n    model=model,  # PeFT модель с LoRA\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    tokenizer=tokenizer,\n    kl_alpha=0.05\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:02:57.655958Z","iopub.execute_input":"2025-09-09T14:02:57.656812Z","iopub.status.idle":"2025-09-09T14:03:01.392201Z","shell.execute_reply.started":"2025-09-09T14:02:57.656774Z","shell.execute_reply":"2025-09-09T14:03:01.391546Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11689a7850084c969ae4c467a2bc6d9e"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## TEMA тренер","metadata":{}},{"cell_type":"code","source":"import copy\nimport torch\nimport torch.nn.functional as F\nfrom trl import SFTTrainer\nfrom peft import PeftModel\n\nclass EMATeacherGPUTrainer(SFTTrainer):\n    \"\"\"\n    EMA teacher trainer (GPU-only).\n    - teacher is a deepcopy of the model, placed on the same device and cast to float32.\n    - EMA updates only floating parameters by name (teacher_param = decay * teacher_param + (1-decay) * student_param).\n    - Designed for non-sharded single-GPU (or standard DataParallel) runs.\n    WARNING: doubles GPU memory usage.\n    \"\"\"\n    def __init__(self, ema_decay=0.999, alpha=1.0, beta=0.05, temperature=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.ema_decay = float(ema_decay)\n        self.alpha = float(alpha)\n        self.beta = float(beta)\n        self.temperature = float(temperature)\n        self.teacher = None\n        self._ema_initialized = False\n        self._student_param_map = None  # name -> param (view into student model)\n\n    def create_teacher(self):\n        if self.teacher is not None:\n            return\n        model_device = next(self.model.parameters()).device\n        # deepcopy ломает PeftModel, поэтому лучше:\n        self.teacher = PeftModel.from_pretrained(self.model.base_model, self.model.peft_config).to(model_device)\n    \n        self.teacher.load_state_dict(self.model.state_dict(), strict=False)\n        for p in self.teacher.parameters():\n            p.data = p.data.to(torch.float32)\n            p.requires_grad_(False)\n        self.teacher.eval()\n        self._student_param_map = dict(self.model.named_parameters())\n        self._ema_initialized = True\n\n\n    def _move_inputs_to_device(self, inputs, device):\n        out = {}\n        for k, v in inputs.items():\n            if torch.is_tensor(v):\n                out[k] = v.to(device)\n            else:\n                out[k] = v\n        return out\n\n    @torch.no_grad()\n    def _update_ema(self):\n        \"\"\"\n        Update teacher parameters with EMA from student.\n        Only updates named parameters that exist in both teacher and student.\n        Teacher params are assumed float32 and on the same device.\n        \"\"\"\n        decay = float(self.ema_decay)\n        if self.teacher is None or self._student_param_map is None:\n            return\n\n        # Iterate teacher named parameters and update from student param with same name\n        for t_name, t_param in self.teacher.named_parameters():\n            s_param = self._student_param_map.get(t_name, None)\n            if s_param is None:\n                # unexpected mismatch; skip\n                continue\n\n            # only update floating point parameters\n            if not torch.is_floating_point(t_param):\n                continue\n\n            # read student param as float32 on teacher device\n            s_data = s_param.detach().to(dtype=t_param.dtype, device=t_param.device)\n            t_param.data.mul_(decay).add_(s_data, alpha=(1-decay))\n\n            # EMA: t = decay * t + (1 - decay) * s\n            # t_param is float32; do in-place ops on t_param.data\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n      # Ensure teacher initialized\n      if not self._ema_initialized:\n          self.create_teacher()\n\n      model_device = next(model.parameters()).device\n      # Prepare inputs for student (do not mutate original)\n      inputs_for_student = {k: (v.to(model_device) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n      if \"labels\" not in inputs_for_student:\n          raise RuntimeError(\"No 'labels' in inputs_for_student — check data collator/tokenization\")\n      labels = inputs_for_student.pop(\"labels\")  # on model_device\n\n      # Student forward\n      outputs = model(**inputs_for_student, labels=labels)\n      ce_loss = outputs.loss\n      logits = outputs.logits  # [B,S,V] on model_device\n\n      # Teacher forward (teacher is on same device in EMATeacherGPUTrainer)\n      teacher_device = next(self.teacher.parameters()).device\n      teacher_inputs = {k: (v.to(teacher_device) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n      teacher_inputs[\"use_cache\"] = False\n      with torch.no_grad():\n          teacher_logits = self.teacher(**teacher_inputs).logits  # [B,S,V] on teacher_device\n\n      # --- Diagnostics: check finite-ness ---\n      if not torch.isfinite(teacher_logits).all():\n        # print(\"[WARN] teacher_logits contains non-finite values (NaN/Inf). Replacing with zeros for stability.\")\n\n        # безопасно получаем min/max, игнорируя NaN\n        finite_mask = torch.isfinite(teacher_logits)\n        if finite_mask.any():\n            t_min = teacher_logits[finite_mask].min().item()\n            t_max = teacher_logits[finite_mask].max().item()\n        else:\n            t_min, t_max = float('nan'), float('nan')\n\n        # print(f\" teacher_logits stats (finite only): min={t_min:.6g}, max={t_max:.6g}\")\n\n        teacher_logits = torch.nan_to_num(\n            teacher_logits,\n            nan=0.0,\n            posinf=1e4,\n            neginf=-1e4\n        )\n\n      # Masking valid tokens\n      valid_mask = labels.ne(-100)  # [B, S]\n      num_valid = int(valid_mask.sum().item())\n      if num_valid == 0:\n          raise ValueError(\"No valid tokens in labels (all -100). Stop training and fix data.\")\n\n      # Select valid positions and cast to float32 for KL computation\n      student_logits_sel = logits[valid_mask].float()         # [N_valid, V] on model_device\n      teacher_logits_sel = teacher_logits.to(logits.device)[valid_mask].float()  # ensure same device\n\n      T = float(self.temperature)\n      V = student_logits_sel.size(-1)\n\n      # Stable teacher log-probs and probs\n      teacher_logp = F.log_softmax(teacher_logits_sel / T, dim=-1)  # may contain large negative numbers\n      # compute teacher_p as exp(teacher_logp) in float32\n      teacher_p = torch.exp(teacher_logp)\n\n      # Option 1: clamp teacher_p to avoid exact zeros (recommended)\n      eps = 1e-12\n      teacher_p = teacher_p.clamp(min=eps)\n      # recompute teacher_logp from clamped teacher_p to keep consistency\n      teacher_logp = torch.log(teacher_p)\n\n      # Optionally apply smoothing to teacher distribution (helps if teacher is extremely peaky)\n      smoothing = 0.0  # set small value like 1e-6..1e-4 to try; here default 0\n      if smoothing > 0.0:\n          # teacher_p <- (1 - s) * teacher_p + s / V\n          teacher_p = teacher_p * (1.0 - smoothing) + (smoothing / float(V))\n          teacher_logp = torch.log(teacher_p.clamp(min=eps))\n\n      # student_logp (already stable because log_softmax)\n      student_logp = F.log_softmax(student_logits_sel / T, dim=-1)\n\n      # Compute per-element KL contribution: teacher_p * (teacher_logp - student_logp)\n      # This is numerically more explicit than relying on F.kl_div under extreme tails.\n      per_token_per_vocab = teacher_p * (teacher_logp - student_logp)  # shape [N_valid, V]\n\n      # Replace NaN/Infs in contributions just in case (shouldn't happen after clamp, but safe)\n      per_token_per_vocab = torch.nan_to_num(per_token_per_vocab, nan=0.0, posinf=0.0, neginf=0.0)\n\n      # Sum over vocab then over tokens\n      kl_sum = per_token_per_vocab.sum()  # scalar\n      kl_sum = kl_sum * (T ** 2)\n\n      kl_loss = kl_sum / float(num_valid)\n\n      # If kl_loss is still nan (very unlikely), fallback to small value and warn\n      if not torch.isfinite(kl_loss):\n          print(\"[WARN] kl_loss is non-finite after stabilization; replacing with 0.0\")\n          kl_loss = torch.tensor(0.0, device=ce_loss.device, dtype=ce_loss.dtype)\n\n      total = self.alpha * ce_loss + self.beta * kl_loss\n\n      # Debug prints\n      ce_f = float(ce_loss.detach().cpu().item())\n      kl_f = float(kl_loss.detach().cpu().item())\n      total_f = float(total.detach().cpu().item())\n      # print(f\"[TRAIN-STEP DEBUG] ce_loss={ce_f:.6g}, kl_loss={kl_f:.6g}, total={total_f:.6g}, num_valid={num_valid}\")\n\n      # EMA update\n      try:\n          with torch.no_grad():\n              self._update_ema()\n      except Exception as e:\n          print(f\"[WARN] EMA update failed: {e}\")\n\n      # Log with trainer\n      try:\n          self.log({\"train/ce_loss\": ce_f, \"train/kl_loss\": kl_f, \"train/total_loss\": total_f, \"train/num_valid\": num_valid})\n      except Exception:\n          pass\n\n      return (total, outputs) if return_outputs else total\n\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = False,\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\n# Инициализация и запуск\ntrainer = EMATeacherGPUTrainer(\n    model=model,  # PeFT модель с LoRA\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    tokenizer=tokenizer,\n    ema_decay=0.999,\n    alpha=1.0, beta=0.05, temperature=1.0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T07:47:42.353192Z","iopub.execute_input":"2025-09-08T07:47:42.353738Z","iopub.status.idle":"2025-09-08T07:47:45.682818Z","shell.execute_reply.started":"2025-09-08T07:47:42.353706Z","shell.execute_reply":"2025-09-08T07:47:45.681742Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afdc54a163d4dc895ca6470d8435bd2"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3126590401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;31m# Инициализация и запуск\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m trainer = EMATeacherGPUTrainer(\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# PeFT модель с LoRA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3126590401.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ema_decay, alpha, beta, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mema_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;31m# - Optimizer and scheduler creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    729\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# At this stage the model is already loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_quantized_and_base_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_model_quantized_and_qat_trainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0;34m\"You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0;34m\" the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"],"ename":"ValueError","evalue":"You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom trl import SFTTrainer\nfrom peft import PeftModel\n\nclass LoRAEMATrainer(SFTTrainer):\n    \"\"\"\n    EMA teacher trainer (GPU-only, optimized for LoRA).\n    - EMA обновляет только веса LoRA (A, B матрицы).\n    - Teacher не хранит всю модель -> только EMA-копию LoRA-параметров.\n    - Это сильно экономит память: нет второй копии всей LLM.\n    \"\"\"\n    def __init__(self, ema_decay=0.999, alpha=1.0, beta=0.05, temperature=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.ema_decay = float(ema_decay)\n        self.alpha = float(alpha)\n        self.beta = float(beta)\n        self.temperature = float(temperature)\n\n        # EMA-хранилище только для LoRA-параметров\n        self.ema_state = {}\n        self._init_ema()\n\n    def _init_ema(self):\n        \"\"\"Сохраняем только LoRA-параметры (только trainable).\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:  # только LoRA\n                self.ema_state[name] = param.detach().clone().float()\n\n    @torch.no_grad()\n    def _update_ema(self):\n        \"\"\"EMA-обновление только LoRA параметров.\"\"\"\n        decay = self.ema_decay\n        for name, param in self.model.named_parameters():\n            if name in self.ema_state:\n                ema_param = self.ema_state[name]\n                # обновляем inplace\n                ema_param.mul_(decay).add_(param.detach().float(), alpha=(1.0 - decay))\n\n    def _apply_ema_weights(self):\n        \"\"\"Создаём копию LoRA state_dict с EMA-весами.\"\"\"\n        ema_lora_state = {}\n        for name, param in self.model.named_parameters():\n            if name in self.ema_state:\n                ema_lora_state[name] = self.ema_state[name].clone().to(param.device)\n        return ema_lora_state\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # ===== Student pass =====\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs, labels=labels)\n        ce_loss = outputs.loss\n        student_logits = outputs.logits\n\n        \n        # ===== Teacher pass (на EMA-весах) =====\n        with torch.no_grad():\n            ema_state = self._apply_ema_weights()\n\n            # временно подменяем веса модели\n            backup = {}\n            for name, param in model.named_parameters():\n                if name in ema_state:\n                    backup[name] = param.data.clone()\n                    param.data.copy_(ema_state[name].to(param.device))\n\n            teacher_logits = model(**inputs, use_cache=False).logits\n\n\n        \n            # откатываем LoRA на студентские веса\n            for name, param in model.named_parameters():\n                if name in backup:\n                    param.data.copy_(backup[name])\n\n        # ===== KL Loss =====\n        valid_mask = labels.ne(-100)\n        student_sel = student_logits[valid_mask].float()\n        teacher_sel = teacher_logits[valid_mask].float()\n\n        T = self.temperature\n        teacher_logp = F.log_softmax(teacher_sel / T, dim=-1)\n        student_logp = F.log_softmax(student_sel / T, dim=-1)\n\n        kl = F.kl_div(student_logp, teacher_logp.exp(), reduction=\"batchmean\") * (T ** 2)\n\n        # total loss\n        total_loss = self.alpha * ce_loss + self.beta * kl\n\n        # EMA update\n        self._update_ema()\n\n        return (total_loss, outputs) if return_outputs else total_loss\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = False,\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\n# Инициализация и запуск\ntrainer = LoRAEMATrainer(\n    model=model,  # PeFT модель с LoRA\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    tokenizer=tokenizer,\n    ema_decay=0.999,\n    alpha=1.0, beta=0.05, temperature=1.0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:20:29.107740Z","iopub.execute_input":"2025-09-09T07:20:29.108457Z","iopub.status.idle":"2025-09-09T07:20:32.648833Z","shell.execute_reply.started":"2025-09-09T07:20:29.108430Z","shell.execute_reply":"2025-09-09T07:20:32.647955Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a856be7f1d4445d390847d17e9c9b2a3"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## FT тренер","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nimport torch\nimport torch.nn as nn\n\n\nclass CombinedLossSFTTrainer(SFTTrainer):\n    def __init__(self, alpha=0.7, beta=0.3, temperature=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.alpha = alpha\n        self.beta = beta\n        self.temperature = temperature\n        # Сохраняем исходные веса модели для teacher\n        self.original_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n        self.teacher = None\n\n    def create_teacher(self):\n        # Создаём единожды глубокую копию текущей модели как teacher\n        if self.teacher is None:\n            import copy\n            self.teacher = copy.deepcopy(self.model)\n            self.teacher.to(self.model.device)\n            self.teacher.eval()\n            for p in self.teacher.parameters():\n                p.requires_grad = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Обеспечиваем teacher\n        self.create_teacher()\n        labels = inputs.pop('labels')\n        # Forward студента\n        outputs = model(**inputs, labels=labels)\n        ce_loss = outputs.loss\n        logits = outputs.logits\n        # Forward teacher без градиентов\n        with torch.no_grad():\n            teacher_logits = self.teacher(**inputs).logits\n        # KL-дивергенция\n        student_lp = nn.functional.log_softmax(logits / self.temperature, dim=-1)\n        teacher_p = nn.functional.softmax(teacher_logits / self.temperature, dim=-1)\n        kl_loss = nn.functional.kl_div(\n            student_lp, teacher_p, reduction='batchmean'\n        ) * (self.temperature ** 2)\n        total = self.alpha * ce_loss + self.beta * kl_loss\n        return (total, outputs) if return_outputs else total\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\nalpha = 1.0\nbeta = 0.05\ntemperature = 1.0\n\n# Инициализация и запуск\ntrainer = CombinedLossSFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    # eval_dataset=eval_dataset,\n    # data_collator=default_data_collator,\n    tokenizer=tokenizer,\n    alpha=alpha,\n    beta=beta,\n    temperature=temperature\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T12:12:30.636947Z","iopub.execute_input":"2025-08-31T12:12:30.637185Z","iopub.status.idle":"2025-08-31T12:12:34.200921Z","shell.execute_reply.started":"2025-08-31T12:12:30.637161Z","shell.execute_reply":"2025-08-31T12:12:34.200062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7a310ab4754508b2a987ffc847c16e"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Обычный тренер","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 5,\n        num_train_epochs = 4,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_torch\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = output_dir,\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:03:09.400701Z","iopub.execute_input":"2025-09-09T14:03:09.401521Z","iopub.status.idle":"2025-09-09T14:23:29.613211Z","shell.execute_reply.started":"2025-09-09T14:03:09.401487Z","shell.execute_reply":"2025-09-09T14:23:29.612470Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,000 | Num Epochs = 4 | Total steps = 1,000\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n \"-____-\"     Trainable parameters = 9,232,384 of 1,552,946,688 (0.59% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 20:10, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.066700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.084700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.497600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.224500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.143600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.087600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.057700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.867000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.959800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.830900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.987200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.879300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.932800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.807600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.766200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.731400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.708100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.802200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.719200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.778000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.762400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.726600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.721100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.699400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.666200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.558400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.623000</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.635700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.629200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.586200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.639500</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.663800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.620200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.656700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.718300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.603400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.650900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.547600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.586700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.567400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.603800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.628100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.648200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.608900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.617500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.580400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.550200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.534300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.570700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.518500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.521100</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.522800</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.504700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.566200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.563200</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.510600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.490500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.501800</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.479300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.545600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.521500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.526600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.537000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.499400</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.488100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.515500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.485000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.525100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.531200</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.540900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.507900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.541400</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.442300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.439700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.455300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.480200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.462200</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.445700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.483600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.447700</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.474600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.461700</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.460100</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.439000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.447600</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.446400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.441500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.447300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.450600</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.490300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.455100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.452800</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.460400</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.464100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.454800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.448300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Сохраняем модель","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:02.003990Z","iopub.execute_input":"2025-09-09T14:24:02.004862Z","iopub.status.idle":"2025-09-09T14:24:02.125642Z","shell.execute_reply.started":"2025-09-09T14:24:02.004832Z","shell.execute_reply":"2025-09-09T14:24:02.125003Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!huggingface-cli login --token $secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:04.363398Z","iopub.execute_input":"2025-09-09T14:24:04.363698Z","iopub.status.idle":"2025-09-09T14:24:05.026778Z","shell.execute_reply.started":"2025-09-09T14:24:04.363675Z","shell.execute_reply":"2025-09-09T14:24:05.025586Z"}},"outputs":[{"name":"stdout","text":"\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `boliboba` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `boliboba`\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:06.733948Z","iopub.execute_input":"2025-09-09T14:24:06.734322Z","iopub.status.idle":"2025-09-09T14:24:11.217539Z","shell.execute_reply.started":"2025-09-09T14:24:06.734287Z","shell.execute_reply":"2025-09-09T14:24:11.216874Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/6.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fffffdc6879846eb8972220f338c14a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9db9fb4a054e9889d897da8160ea9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8efbb1801d3b49349dc917c160a2def0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/37.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e119cfcf8ce436b95c60e412c450bd4"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/DimaSK1/Qwen2-1.5B-bnb-4bit-kl-sft-base/commit/59b3499dad2d85b6a58c2779b75d68ce5772807a', commit_message='End of training', commit_description='', oid='59b3499dad2d85b6a58c2779b75d68ce5772807a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/DimaSK1/Qwen2-1.5B-bnb-4bit-kl-sft-base', endpoint='https://huggingface.co', repo_type='model', repo_id='DimaSK1/Qwen2-1.5B-bnb-4bit-kl-sft-base'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# ОБУЧЕНИЕ ПОСЛЕДУЮЩИХ МОДЕЛЕЙ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade transformers accelerate peft unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:07:50.808182Z","iopub.execute_input":"2025-09-02T14:07:50.808482Z","iopub.status.idle":"2025-09-02T14:07:54.990812Z","shell.execute_reply.started":"2025-09-02T14:07:50.808452Z","shell.execute_reply":"2025-09-02T14:07:54.989750Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\n\nmax_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\noutput_dir = 'Qwen2-0.5B-bnb-4bit-sft-5' # название модели после чикла обучения\n\n# model_name = \"DimaSK1/gemma_2b_bnb_4bit_medical_base\"\nmodel_name = \"DimaSK1/Qwen2-0.5B-bnb-4bit-sft-4\"\n\nmodel_new, tokenizer_new = FastLanguageModel.from_pretrained(\n    model_name = model_name, # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    # dtype = dtype,\n    load_in_4bit = load_in_4bit,\n) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:03:31.753708Z","iopub.execute_input":"2025-09-03T11:03:31.754225Z","iopub.status.idle":"2025-09-03T11:04:28.510639Z","shell.execute_reply.started":"2025-09-03T11:03:31.754195Z","shell.execute_reply":"2025-09-03T11:04:28.510056Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-09-03 11:03:42.065057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756897422.411150      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756897422.509080      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.56.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/457M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1ca937723741cca360ba12d767d4c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b9df681a2a4b8ca69c6b081ab8adc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee6d334275a4f80aa7afe27018eba7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ff43e7c98546edb585776b85b58fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634c5b2c429d4c79aa010fd4232a329b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/107 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7249fa7b6f44f7af43c3f29a105329"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d46b1c7f6ef4b0e855026e6e1d2c7b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ace98b528884eddb3cc48a68dc050c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/17.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22c740ad227b4e638b57ba4c695492c2"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.8.10 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n# OUTPUT_DIR = 'gemma_2b_bnb_sft_good_bad_1'\ntrain_data_good = pd.read_excel(\"/kaggle/input/g4-gen-qwen0-5-sft/g4_data_gen_qwen0.5_sft.xlsx\")\ntrain_data_bad = pd.read_excel(\"/kaggle/input/g4-gen-qwen0-5-base/g4_data_gen_qwen0.5_base.xlsx\")\n# data = train_data\ntrain_data = pd.concat([train_data_good, train_data_bad])\ntrain_data = train_data[['description', 'result']]\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\ntrain_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:04:28.511291Z","iopub.execute_input":"2025-09-03T11:04:28.511521Z","iopub.status.idle":"2025-09-03T11:04:29.289036Z","shell.execute_reply.started":"2025-09-03T11:04:28.511502Z","shell.execute_reply":"2025-09-03T11:04:29.288266Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           description  \\\n0    пунктирован узел коллоидного в разной степени ...   \n1    п 38 в мазке  неравномерной толщины с обильным...   \n2    Л12: в препарате на фоне коллоидаопределяются ...   \n3    Л 14 (№3424/22). Пунктирован узловой коллоидны...   \n4    Л 14 (№4284/20). Пунктирован узловой коллоидын...   \n..                                                 ...   \n995  Пер 19 (№3258/20 1,2 - 2 мазка). На фоне элеме...   \n996  П 17 (№3728/200. На фоне коллоида и измененных...   \n997  П45 В мазке очень высокой клеточности  неравно...   \n998  л32 в мазке неравномерной толщины с обильным п...   \n999  В препарате на фоне кистозно-геморрагического ...   \n\n                                                result  \n0                              Bethesda - категория II  \n1    В соответствии с критериями системы классифика...  \n2    Для каждого клетки фолликулярного эпителия в п...  \n3    Доброкачественное образование в соответствии с...  \n4    Доброкачественное образование в соответствии с...  \n..                                                 ...  \n995  Доброкачественное образование в соответствии с...  \n996  Доброкачественное образование в соответствии с...  \n997  П45 В мазке очень высокой клеточности  неравно...  \n998  В соответствии с критериями системы классифика...  \n999  Доброкачественное образование в соответствии с...  \n\n[1000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>пунктирован узел коллоидного в разной степени ...</td>\n      <td>Bethesda - категория II</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>п 38 в мазке  неравномерной толщины с обильным...</td>\n      <td>В соответствии с критериями системы классифика...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Л12: в препарате на фоне коллоидаопределяются ...</td>\n      <td>Для каждого клетки фолликулярного эпителия в п...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Л 14 (№3424/22). Пунктирован узловой коллоидны...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Л 14 (№4284/20). Пунктирован узловой коллоидын...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>Пер 19 (№3258/20 1,2 - 2 мазка). На фоне элеме...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>П 17 (№3728/200. На фоне коллоида и измененных...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>П45 В мазке очень высокой клеточности  неравно...</td>\n      <td>П45 В мазке очень высокой клеточности  неравно...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>л32 в мазке неравномерной толщины с обильным п...</td>\n      <td>В соответствии с критериями системы классифика...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>В препарате на фоне кистозно-геморрагического ...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\ntrain_data.to_excel('g4_train_qwen0.5_sft.xlsx', index=False)\n# train_data1 = pd.read_excel('/kaggle/input/g1-train-qwen-tema/g1_train_qwen_tema.xlsx')\ntrain_data1 = train_data[['description', 'result']]\ntrain_data1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:04:29.290779Z","iopub.execute_input":"2025-09-03T11:04:29.291334Z","iopub.status.idle":"2025-09-03T11:04:29.475148Z","shell.execute_reply.started":"2025-09-03T11:04:29.291315Z","shell.execute_reply":"2025-09-03T11:04:29.474401Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                           description  \\\n0    пунктирован узел коллоидного в разной степени ...   \n1    п 38 в мазке  неравномерной толщины с обильным...   \n2    Л12: в препарате на фоне коллоидаопределяются ...   \n3    Л 14 (№3424/22). Пунктирован узловой коллоидны...   \n4    Л 14 (№4284/20). Пунктирован узловой коллоидын...   \n..                                                 ...   \n995  Пер 19 (№3258/20 1,2 - 2 мазка). На фоне элеме...   \n996  П 17 (№3728/200. На фоне коллоида и измененных...   \n997  П45 В мазке очень высокой клеточности  неравно...   \n998  л32 в мазке неравномерной толщины с обильным п...   \n999  В препарате на фоне кистозно-геморрагического ...   \n\n                                                result  \n0                              Bethesda - категория II  \n1    В соответствии с критериями системы классифика...  \n2    Для каждого клетки фолликулярного эпителия в п...  \n3    Доброкачественное образование в соответствии с...  \n4    Доброкачественное образование в соответствии с...  \n..                                                 ...  \n995  Доброкачественное образование в соответствии с...  \n996  Доброкачественное образование в соответствии с...  \n997  П45 В мазке очень высокой клеточности  неравно...  \n998  В соответствии с критериями системы классифика...  \n999  Доброкачественное образование в соответствии с...  \n\n[1000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>пунктирован узел коллоидного в разной степени ...</td>\n      <td>Bethesda - категория II</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>п 38 в мазке  неравномерной толщины с обильным...</td>\n      <td>В соответствии с критериями системы классифика...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Л12: в препарате на фоне коллоидаопределяются ...</td>\n      <td>Для каждого клетки фолликулярного эпителия в п...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Л 14 (№3424/22). Пунктирован узловой коллоидны...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Л 14 (№4284/20). Пунктирован узловой коллоидын...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>Пер 19 (№3258/20 1,2 - 2 мазка). На фоне элеме...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>П 17 (№3728/200. На фоне коллоида и измененных...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>П45 В мазке очень высокой клеточности  неравно...</td>\n      <td>П45 В мазке очень высокой клеточности  неравно...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>л32 в мазке неравномерной толщины с обильным п...</td>\n      <td>В соответствии с критериями системы классифика...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>В препарате на фоне кистозно-геморрагического ...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"X_train = train_data\n\nalpaca_prompt = \"\"\"\n### Input:\n{}\n\n### Output:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer_new.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    inputs = examples['description']\n    outputs = examples['result']\n    texts = []\n    for input, output in zip(inputs, outputs):\n        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\nfrom datasets import Dataset,DatasetDict\ntrain_dataset_dict = DatasetDict({\n    \"train\": Dataset.from_pandas(X_train),\n})\n\ntrain_dataset_dict = train_dataset_dict.map(formatting_prompts_func, batched = True,)\ntrain_dataset_dict['train'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:04:29.475951Z","iopub.execute_input":"2025-09-03T11:04:29.476224Z","iopub.status.idle":"2025-09-03T11:04:29.559998Z","shell.execute_reply.started":"2025-09-03T11:04:29.476198Z","shell.execute_reply":"2025-09-03T11:04:29.559207Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd5d63f18ba415191ae01bcf1ab3659"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'description': 'п 38 в мазке  неравномерной толщины с обильным плотным коллоидом и с  большой примесью крови цитограмма характерная для коллоидного в разной степени пролиферирующего зоба с     участками аденоматоза и выраженными  дегенеративными изменениями.',\n 'result': 'В соответствии с критериями системы классификации Bethesda: доброкачественное образование щитовидной железыдиагностическая категория II.',\n 'text': '\\n### Input:\\nп 38 в мазке  неравномерной толщины с обильным плотным коллоидом и с  большой примесью крови цитограмма характерная для коллоидного в разной степени пролиферирующего зоба с     участками аденоматоза и выраженными  дегенеративными изменениями.\\n\\n### Output:\\nВ соответствии с критериями системы классификации Bethesda: доброкачественное образование щитовидной железыдиагностическая категория II.<|endoftext|>'}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom trl import SFTTrainer\nfrom transformers.trainer import Accelerator\nimport warnings\n\nclass CustomSFTTrainer_w_kl(SFTTrainer):\n    def __init__(self, model, args, train_dataset, dataset_text_field, data_collator=None, processing_class=None, kl_alpha=0.1):\n        \n        super().__init__(model=model,\n                         args=args,\n                         train_dataset=train_dataset,\n                         dataset_text_field=dataset_text_field,\n                         data_collator=data_collator,\n                         processing_class=processing_class)\n\n        if not hasattr(model, 'disable_adapter'):\n            warnings.warn(\n                \"The model provided does not appear to be a PEFT model with a LoRA adapter. \"\n                \"The KL divergence loss term will not be calculated and the trainer will fall back to standard SFT.\"\n            )\n\n        self.accelerator = Accelerator()\n        self._total_train_tokens = 0\n        # After super().__init__, self._metrics is initialized by the parent Trainer class.\n        # We need to add our custom metric to it.\n        if \"kl_loss\" not in self._metrics[\"train\"]:\n            self._metrics[\"train\"][\"kl_loss\"] = []\n        if \"eval\" in self._metrics and \"kl_loss\" not in self._metrics[\"eval\"]:\n            self._metrics[\"eval\"][\"kl_loss\"] = []\n        self.kl_alpha = kl_alpha\n\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Computes SFT loss and adds a KL divergence penalty term.\n        The reference log probabilities are computed on the fly by disabling the LoRA adapter.\n        \"\"\"\n        if self.model_accepts_loss_kwargs:\n            loss_kwargs = {}\n            if num_items_in_batch is not None:\n                loss_kwargs[\"num_items_in_batch\"] = num_items_in_batch\n        # Compute standard SFT loss with the adapter enabled\n        sft_loss, outputs = super().compute_loss(model, inputs, return_outputs=True, **loss_kwargs)\n\n        # If the model is not a PEFT model, we can't compute KL loss, so we return SFT loss\n        if not hasattr(model, 'disable_adapter'):\n            warnings.warn(\n                \"The model provided does not appear to be a PEFT model with a LoRA adapter. \"\n                \"The KL divergence loss term will not be calculated and the trainer will fall back to standard SFT.\"\n            )\n            return (sft_loss, outputs) if return_outputs else sft_loss\n\n        # Get the logits from the adapter-enabled model\n        logits = outputs.logits\n        labels = inputs[\"labels\"]\n\n        # Compute reference logits by disabling the adapter\n        with torch.no_grad(), model.disable_adapter():\n            ref_outputs = model(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs.get(\"attention_mask\", None),\n            )\n        ref_logits = ref_outputs.logits\n\n        # Shift all tensors for next-token prediction\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        shift_ref_logits = ref_logits[..., :-1, :].contiguous()\n\n        # Create a mask for the answer tokens (non -100 labels)\n        answer_mask = (shift_labels != -100).float()\n\n        # Calculate log probabilities from the adapter model and reference model logits\n        log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n        ref_log_probs = torch.nn.functional.log_softmax(shift_ref_logits, dim=-1)\n\n        # Calculate KL divergence: KL(P_ref || P_model) = sum(P_ref * (log P_ref - log P_model))\n        kl_divergence = torch.exp(ref_log_probs) * (ref_log_probs - log_probs)\n\n        # Sum over the vocabulary dimension and apply the mask and average over the sequence and batch\n        kl_loss_per_token = (kl_divergence.sum(dim=-1) * answer_mask)\n        kl_loss = kl_loss_per_token.sum() / answer_mask.sum()\n\n        # Combine losses\n        loss = sft_loss + self.kl_alpha * kl_loss\n\n        # Logging\n        self._metrics[\"train\"][\"kl_loss\"].append(kl_loss.item())\n\n        return (loss, outputs) if return_outputs else loss\n\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    # fp16 = False,\n    # bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\ntrainer = CustomSFTTrainer_w_kl(\n    model=model_new,\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    kl_alpha=0.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nimport torch\nimport torch.nn.functional as F\nfrom trl import SFTTrainer\n\nclass EMATeacherGPUTrainer(SFTTrainer):\n    \"\"\"\n    EMA teacher trainer (GPU-only).\n    - teacher is a deepcopy of the model, placed on the same device and cast to float32.\n    - EMA updates only floating parameters by name (teacher_param = decay * teacher_param + (1-decay) * student_param).\n    - Designed for non-sharded single-GPU (or standard DataParallel) runs.\n    WARNING: doubles GPU memory usage.\n    \"\"\"\n    def __init__(self, ema_decay=0.999, alpha=1.0, beta=0.05, temperature=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.ema_decay = float(ema_decay)\n        self.alpha = float(alpha)\n        self.beta = float(beta)\n        self.temperature = float(temperature)\n        self.teacher = None\n        self._ema_initialized = False\n        self._student_param_map = None  # name -> param (view into student model)\n\n    def create_teacher(self):\n        if self.teacher is not None:\n            return\n\n        # Deep-copy the student model and place teacher on same device as student.\n        # Cast teacher to float32 for stable EMA.\n        model_device = next(self.model.parameters()).device\n        self.teacher = copy.deepcopy(self.model).to(model_device)\n\n        # Cast teacher parameters to float32 (in-place)\n        for p in self.teacher.parameters():\n            if p.dtype != torch.float32:\n                p.data = p.data.to(torch.float32)\n\n        self.teacher.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n\n        # Build a mapping name -> student parameter for quick EMA updates.\n        # Note: this holds references to the student parameters (they will change during training).\n        self._student_param_map = dict(self.model.named_parameters())\n\n        self._ema_initialized = True\n\n    def _move_inputs_to_device(self, inputs, device):\n        out = {}\n        for k, v in inputs.items():\n            if torch.is_tensor(v):\n                out[k] = v.to(device)\n            else:\n                out[k] = v\n        return out\n\n    @torch.no_grad()\n    def _update_ema(self):\n        \"\"\"\n        Update teacher parameters with EMA from student.\n        Only updates named parameters that exist in both teacher and student.\n        Teacher params are assumed float32 and on the same device.\n        \"\"\"\n        decay = float(self.ema_decay)\n        if self.teacher is None or self._student_param_map is None:\n            return\n\n        # Iterate teacher named parameters and update from student param with same name\n        for t_name, t_param in self.teacher.named_parameters():\n            s_param = self._student_param_map.get(t_name, None)\n            if s_param is None:\n                # unexpected mismatch; skip\n                continue\n\n            # only update floating point parameters\n            if not torch.is_floating_point(t_param):\n                continue\n\n            # read student param as float32 on teacher device\n            s_data = s_param.data.detach().to(torch.float32).to(t_param.device)\n\n            # EMA: t = decay * t + (1 - decay) * s\n            # t_param is float32; do in-place ops on t_param.data\n            t_param.data.mul_(decay)\n            t_param.data.add_(s_data * (1.0 - decay))\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n      # Ensure teacher initialized\n      if not self._ema_initialized:\n          self.create_teacher()\n\n      model_device = next(model.parameters()).device\n      # Prepare inputs for student (do not mutate original)\n      inputs_for_student = {k: (v.to(model_device) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n      if \"labels\" not in inputs_for_student:\n          raise RuntimeError(\"No 'labels' in inputs_for_student — check data collator/tokenization\")\n      labels = inputs_for_student.pop(\"labels\")  # on model_device\n\n      # Student forward\n      outputs = model(**inputs_for_student, labels=labels)\n      ce_loss = outputs.loss\n      logits = outputs.logits  # [B,S,V] on model_device\n\n      # Teacher forward (teacher is on same device in EMATeacherGPUTrainer)\n      teacher_device = next(self.teacher.parameters()).device\n      teacher_inputs = {k: (v.to(teacher_device) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n      teacher_inputs[\"use_cache\"] = False\n      with torch.no_grad():\n          teacher_logits = self.teacher(**teacher_inputs).logits # [B,S,V] on teacher_device\n          # print(not torch.isfinite(teacher_logits).all())\n        \n      # # --- Если teacher_logits = None → создаём фиктивный тензор NaN ---\n      # if teacher_logits is None:\n      #     B, L = inputs[\"input_ids\"].shape\n      #     V = model.config.vocab_size\n      #     teacher_logits = torch.full((B, L, V), float(\"nan\"), device=model.device)\n        \n      #   # --- Стабилизируем ---\n      #     teacher_logits = torch.nan_to_num(\n      #           teacher_logits,\n      #           nan=0.0,\n      #           posinf=1e4,\n      #           neginf=-1e4,\n      #       )\n      # --- Diagnostics: check finite-ness ---\n      if not torch.isfinite(teacher_logits).all():\n        # print(\"[WARN] teacher_logits contains non-finite values (NaN/Inf). Replacing with zeros for stability.\")\n\n        # безопасно получаем min/max, игнорируя NaN\n        finite_mask = torch.isfinite(teacher_logits)\n        if finite_mask.any():\n            t_min = teacher_logits[finite_mask].min().item()\n            t_max = teacher_logits[finite_mask].max().item()\n        else:\n            t_min, t_max = float('nan'), float('nan')\n\n        # print(f\" teacher_logits stats (finite only): min={t_min:.6g}, max={t_max:.6g}\")\n\n        teacher_logits = torch.nan_to_num(\n            teacher_logits,\n            nan=0.0,\n            posinf=1e4,\n            neginf=-1e4\n        )\n\n      # Masking valid tokens\n      valid_mask = labels.ne(-100)  # [B, S]\n      num_valid = int(valid_mask.sum().item())\n      if num_valid == 0:\n          raise ValueError(\"No valid tokens in labels (all -100). Stop training and fix data.\")\n\n      # Select valid positions and cast to float32 for KL computation\n      student_logits_sel = logits[valid_mask].float()         # [N_valid, V] on model_device\n      teacher_logits_sel = teacher_logits.to(logits.device)[valid_mask].float()  # ensure same device\n\n      T = float(self.temperature)\n      V = student_logits_sel.size(-1)\n\n      # Stable teacher log-probs and probs\n      teacher_logp = F.log_softmax(teacher_logits_sel / T, dim=-1)  # may contain large negative numbers\n      # compute teacher_p as exp(teacher_logp) in float32\n      teacher_p = torch.exp(teacher_logp)\n\n      # Option 1: clamp teacher_p to avoid exact zeros (recommended)\n      eps = 1e-12\n      teacher_p = teacher_p.clamp(min=eps)\n      # recompute teacher_logp from clamped teacher_p to keep consistency\n      teacher_logp = torch.log(teacher_p)\n\n      # Optionally apply smoothing to teacher distribution (helps if teacher is extremely peaky)\n      smoothing = 0.0  # set small value like 1e-6..1e-4 to try; here default 0\n      if smoothing > 0.0:\n          # teacher_p <- (1 - s) * teacher_p + s / V\n          teacher_p = teacher_p * (1.0 - smoothing) + (smoothing / float(V))\n          teacher_logp = torch.log(teacher_p.clamp(min=eps))\n\n      # student_logp (already stable because log_softmax)\n      student_logp = F.log_softmax(student_logits_sel / T, dim=-1)\n\n      # Compute per-element KL contribution: teacher_p * (teacher_logp - student_logp)\n      # This is numerically more explicit than relying on F.kl_div under extreme tails.\n      per_token_per_vocab = teacher_p * (teacher_logp - student_logp)  # shape [N_valid, V]\n\n      # Replace NaN/Infs in contributions just in case (shouldn't happen after clamp, but safe)\n      per_token_per_vocab = torch.nan_to_num(per_token_per_vocab, nan=0.0, posinf=0.0, neginf=0.0)\n\n      # Sum over vocab then over tokens\n      kl_sum = per_token_per_vocab.sum()  # scalar\n      kl_sum = kl_sum * (T ** 2)\n\n      kl_loss = kl_sum / float(num_valid)\n\n      # If kl_loss is still nan (very unlikely), fallback to small value and warn\n      if not torch.isfinite(kl_loss):\n          print(\"[WARN] kl_loss is non-finite after stabilization; replacing with 0.0\")\n          kl_loss = torch.tensor(0.0, device=ce_loss.device, dtype=ce_loss.dtype)\n\n      total = self.alpha * ce_loss + self.beta * kl_loss\n\n      # Debug prints\n      ce_f = float(ce_loss.detach().cpu().item())\n      kl_f = float(kl_loss.detach().cpu().item())\n      total_f = float(total.detach().cpu().item())\n      # print(f\"[TRAIN-STEP DEBUG] ce_loss={ce_f:.6g}, kl_loss={kl_f:.6g}, total={total_f:.6g}, num_valid={num_valid}\")\n\n      # EMA update\n      try:\n          with torch.no_grad():\n              self._update_ema()\n      except Exception as e:\n          print(f\"[WARN] EMA update failed: {e}\")\n\n      # Log with trainer\n      try:\n          self.log({\"train/ce_loss\": ce_f, \"train/kl_loss\": kl_f, \"train/total_loss\": total_f, \"train/num_valid\": num_valid})\n      except Exception:\n          pass\n\n      return (total, outputs) if return_outputs else total\n\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = False,\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\n# Инициализация и запуск\ntrainer = EMATeacherGPUTrainer(\n    model=model_new,  # PeFT модель с LoRA\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    tokenizer=tokenizer_new,\n    ema_decay=0.999,\n    alpha=1.0, beta=0.05, temperature=1.0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:40:51.473243Z","iopub.execute_input":"2025-09-02T14:40:51.473932Z","iopub.status.idle":"2025-09-02T14:40:55.227019Z","shell.execute_reply.started":"2025-09-02T14:40:51.473908Z","shell.execute_reply":"2025-09-02T14:40:55.226050Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d275fa3ffe54210ae7f7cef19fb2d2c"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Тренер с комбинированным лоссом для обучения","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nimport torch\nimport torch.nn as nn\n\n\nclass CombinedLossSFTTrainer(SFTTrainer):\n    def __init__(self, alpha=0.7, beta=0.3, temperature=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.alpha = alpha\n        self.beta = beta\n        self.temperature = temperature\n        # Сохраняем исходные веса модели для teacher\n        self.original_state = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n        self.teacher = None\n\n    def create_teacher(self):\n        # Создаём единожды глубокую копию текущей модели как teacher\n        if self.teacher is None:\n            import copy\n            self.teacher = copy.deepcopy(self.model)\n            self.teacher.to(self.model.device)\n            self.teacher.eval()\n            for p in self.teacher.parameters():\n                p.requires_grad = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Обеспечиваем teacher\n        self.create_teacher()\n        labels = inputs.pop('labels')\n        # Forward студента\n        outputs = model(**inputs, labels=labels)\n        ce_loss = outputs.loss\n        logits = outputs.logits\n        # Forward teacher без градиентов\n        with torch.no_grad():\n            teacher_logits = self.teacher(**inputs).logits\n        # KL-дивергенция\n        student_lp = nn.functional.log_softmax(logits / self.temperature, dim=-1)\n        teacher_p = nn.functional.softmax(teacher_logits / self.temperature, dim=-1)\n        kl_loss = nn.functional.kl_div(\n            student_lp, teacher_p, reduction='batchmean'\n        ) * (self.temperature ** 2)\n        total = self.alpha * ce_loss + self.beta * kl_loss\n        return (total, outputs) if return_outputs else total\n\nfrom transformers import TrainingArguments\n# Аргументы тренировки\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 1,\n    # per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps = 2,\n    warmup_steps = 5,\n    num_train_epochs = 4,\n    learning_rate = 2e-4,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 10,\n    optim = \"adamw_torch\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = output_dir,\n    report_to = \"none\" # Use this for WandB etc\n)\n\nalpha = 1.0\nbeta = 0.05\ntemperature = 1.0\n\n# Инициализация и запуск\ntrainer = CombinedLossSFTTrainer(\n    model=model_new,\n    args=training_args,\n    train_dataset=train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    # eval_dataset=eval_dataset,\n    # data_collator=default_data_collator,\n    tokenizer=tokenizer_new,\n    alpha=alpha,\n    beta=beta,\n    temperature=temperature\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T14:17:56.993659Z","iopub.execute_input":"2025-08-31T14:17:56.993953Z","iopub.status.idle":"2025-08-31T14:18:00.418025Z","shell.execute_reply.started":"2025-08-31T14:17:56.993929Z","shell.execute_reply":"2025-08-31T14:18:00.417344Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6d625918f614c7a8d6b55d4f2f739a4"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Стандартный тренер","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model_new,\n    tokenizer = tokenizer_new,\n    train_dataset = train_dataset_dict['train'],\n    dataset_text_field = \"text\",\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 5,\n        num_train_epochs = 4,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 10,\n        optim = \"adamw_torch\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = output_dir,\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:04:29.560781Z","iopub.execute_input":"2025-09-03T11:04:29.561027Z","iopub.status.idle":"2025-09-03T11:04:33.150895Z","shell.execute_reply.started":"2025-09-03T11:04:29.561002Z","shell.execute_reply":"2025-09-03T11:04:33.150230Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ee6d75dc914d7f911e161ee3feeaed"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\n\n\nos.environ['UNSLOTH_RETURN_LOGITS'] = '1'\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:04:33.151899Z","iopub.execute_input":"2025-09-03T11:04:33.152211Z","iopub.status.idle":"2025-09-03T11:16:33.925656Z","shell.execute_reply.started":"2025-09-03T11:04:33.152175Z","shell.execute_reply":"2025-09-03T11:16:33.924858Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,000 | Num Epochs = 4 | Total steps = 1,000\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n \"-____-\"     Trainable parameters = 4,399,104 of 498,431,872 (0.88% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 11:52, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>entropy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.340600</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.288000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.245600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.351000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.363800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.322900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.324100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.352400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.295600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.328000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.295100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.307300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.306700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.361600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.344900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.294900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.312900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.302600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.280000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.314700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.269000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.286000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.327400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.357300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.282000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.234800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.230600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.226700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.224200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.245900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.231100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.281100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.244000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.216300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.217800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.238800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.245300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.253000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.249500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.253400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.229200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.234100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.256600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.235700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.240700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.268900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.239800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.205900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.212400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.209500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.169800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.185500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.188300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.183700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.160200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.204200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.199800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.184000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.157900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.197800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.173600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.158000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.190000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.175200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.203800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.178600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.187100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.187800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.197800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.186700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.175600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.171200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.158300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.191100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.179100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.152800</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.151400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.136300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.160100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.133400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.138500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.146000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.130600</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.140500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.161700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.146900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.148900</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.142500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.160200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.154700</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.135500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.142500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.136400</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.121300</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.153000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.136100</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.121500</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.122000</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.138200</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.143400</td>\n      <td>No Log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:20:23.987502Z","iopub.execute_input":"2025-09-03T11:20:23.988255Z","iopub.status.idle":"2025-09-03T11:20:24.073206Z","shell.execute_reply.started":"2025-09-03T11:20:23.988226Z","shell.execute_reply":"2025-09-03T11:20:24.072664Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!huggingface-cli login --token $secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:20:25.852646Z","iopub.execute_input":"2025-09-03T11:20:25.853138Z","iopub.status.idle":"2025-09-03T11:20:26.514408Z","shell.execute_reply.started":"2025-09-03T11:20:25.853112Z","shell.execute_reply":"2025-09-03T11:20:26.513624Z"}},"outputs":[{"name":"stdout","text":"\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `boliboba` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `boliboba`\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:20:30.193079Z","iopub.execute_input":"2025-09-03T11:20:30.193827Z","iopub.status.idle":"2025-09-03T11:20:36.976327Z","shell.execute_reply.started":"2025-09-03T11:20:30.193793Z","shell.execute_reply":"2025-09-03T11:20:36.975564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbdf135256d84a0198ce0fcb832035dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/17.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907230fa5f9a46f58a12d9838a947a10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e613774152c4491fa7437191a22e35b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/6.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb7abcbd7a845f3a7b812117bc397b2"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/DimaSK1/Qwen2-0.5B-bnb-4bit-sft-5/commit/6f2c58d315c2398e7998af33fea4552042fb8aeb', commit_message='End of training', commit_description='', oid='6f2c58d315c2398e7998af33fea4552042fb8aeb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/DimaSK1/Qwen2-0.5B-bnb-4bit-sft-5', endpoint='https://huggingface.co', repo_type='model', repo_id='DimaSK1/Qwen2-0.5B-bnb-4bit-sft-5'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Генерация ответов для обучения","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade unsloth transformers accelerate peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:27:18.118066Z","iopub.execute_input":"2025-09-10T13:27:18.118671Z","iopub.status.idle":"2025-09-10T13:32:16.534163Z","shell.execute_reply.started":"2025-09-10T13:27:18.118637Z","shell.execute_reply":"2025-09-10T13:32:16.533243Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\n# import os\n# os.environ[\"UNSLOTH_DISABLE_RL_PATCH\"] = \"1\"\nfrom unsloth import FastLanguageModel\n\n\nmax_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# model_name = \"unsloth/Qwen2-0.5B-bnb-4bit\"\n# model_name = \"DimaSK1/Qwen2-1.5B-bnb-4bit_1\"\nmodel_name = 'DimaSK1/Qwen2-0.5B-bnb-4bit-kl-sft-base'\n\n\n\nmodel_new, tokenizer_new = FastLanguageModel.from_pretrained(\n    model_name = model_name, # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    load_in_4bit = load_in_4bit,\n) # Enable native 2x faster inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:33:25.614638Z","iopub.execute_input":"2025-09-10T13:33:25.615362Z","iopub.status.idle":"2025-09-10T13:34:10.518583Z","shell.execute_reply.started":"2025-09-10T13:33:25.615323Z","shell.execute_reply":"2025-09-10T13:34:10.517682Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-09-10 13:33:35.046109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757511215.376607      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757511215.472818      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3098169060.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model_new, tokenizer_new = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# YOUR MODEL YOU USED FOR TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m#     dispatch_model = FastGraniteModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             return FastModel.from_pretrained(\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mmodel_name\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mold_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mmax_seq_length\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, qat_scheme, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mredirector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mpatch_loss_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             model_types, supports_sdpa = unsloth_compile_transformers(\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0mdtype\u001b[0m                   \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0mmodel_name\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(dtype, model_name, model_types, token, revision, trust_remote_code, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, unsloth_force_compile)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0msupports_sdpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         _unsloth_compile_transformers(\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             \u001b[0msdpa_dynamic_mask\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0msdpa_dynamic_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36munsloth_compile_transformers\u001b[0;34m(model_type, sdpa_dynamic_mask, sdpa_bool_masks, sdpa_gqa_replace, sdpa_dynamic_compile, compile_attention, disable_causal_masks, compile_torch_modules, compile_custom_modules, compile_function_calls, fuse_lm_head, gradient_checkpointing, manual_replacements, fast_lora_forwards, fast_residual_stream, accurate_accumulation, epilogue_fusion, max_autotune, shape_padding, cudagraphs, debug, fullgraph, import_from_cache, disable, return_logits, supports_sdpa)\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisable_causal_masks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_location}.{module}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_update_causal_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'transformers.models.bit.modeling_bit' has no attribute 'Linear'"],"ename":"AttributeError","evalue":"module 'transformers.models.bit.modeling_bit' has no attribute 'Linear'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n\ntest_data1 = pd.read_excel('/kaggle/input/dataf1/data 1.xlsx')\n# test_data1= test_data1[:100]\n# test_data1 = test_data1[950:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T12:14:32.014518Z","iopub.execute_input":"2025-09-10T12:14:32.015224Z","iopub.status.idle":"2025-09-10T12:14:32.738991Z","shell.execute_reply.started":"2025-09-10T12:14:32.015199Z","shell.execute_reply":"2025-09-10T12:14:32.738384Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"alpaca_prompt = \"\"\"\n### Input:\n{}\n\n### Output:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer_new.eos_token # Must add EOS_TOKEN\n\ndef prompts_func(examples):\n    inputs = examples[\"description\"]\n    texts = []\n    for input in inputs:\n        output = ''\n        text = alpaca_prompt.format(input, output)\n        texts.append(text)\n    return { \"text\" : texts, }\n\n\nX_test = test_data1[['description', 'target']]\ntext_data = prompts_func(X_test)\ntest_data = pd.DataFrame(data=text_data)\ntarget = X_test['target'].to_list()\n\nnum_samples = test_data.shape[0]\nnum_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T12:14:34.321199Z","iopub.execute_input":"2025-09-10T12:14:34.322286Z","iopub.status.idle":"2025-09-10T12:14:34.352988Z","shell.execute_reply.started":"2025-09-10T12:14:34.322259Z","shell.execute_reply":"2025-09-10T12:14:34.352412Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1000"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_data['text'][0] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T08:03:59.669736Z","iopub.execute_input":"2025-09-09T08:03:59.670061Z","iopub.status.idle":"2025-09-09T08:03:59.675586Z","shell.execute_reply.started":"2025-09-09T08:03:59.670023Z","shell.execute_reply":"2025-09-09T08:03:59.674905Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\n### Input:\\nП 20 (№7053/22 1). Пунктирован коллоидный зоб с морфологическими признаками регрессивных изменений и небольшим содержанием в мазке дистрофичных тиреоцитов на фоне коллоида и измененных эритроцитов. Л 20 (№7053/22 2). Пунктирован узловой коллоидный зоб с кистозно-регрессивными изменениямиэлементами воспалительной инфильтрации и небольшим содержанием в мазке дистрофичных клеток фолликулярного эпителия щитовидной железы.\\n\\n### Output:\\n'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom unsloth import FastLanguageModel\nimport numpy as np\n\n\n\n# Убедитесь, что модель переведена в режим инференса\nFastLanguageModel.for_inference(model_new)\n\nresults = []\nperplexities1 = []\nperplexities2 = []\ntoken_probabilities = {}\n\n# Функция для вычисления перплексии и вероятностей для сгенерированного текста\ndef calculate_perplexity_and_generated_probabilities(model, tokenizer, test_data, num_samples=1000):\n    model.eval()  # Переводим модель в режим оценки\n    total_log_prob = 0.0\n    total_words = 0\n\n    with torch.no_grad():\n        for i in tqdm(range(num_samples)):\n            tkn_probs = []\n            tkn_distr_probs = []\n            # Получаем токенизированный текст\n            inputs = tokenizer(test_data['text'][i], return_tensors='pt', truncation=True, max_length=1024).to('cuda')\n\n            # Генерация текста с возвратом вероятностей\n            output = model.generate(\n                **inputs,\n                max_new_tokens=128,\n                use_cache=False,\n                # output_scores=True,\n                # return_dict_in_generate=True\n            )\n            \n            generated_ids = output[:, inputs.input_ids.shape[-1]:]  # Только сгенерированные токены\n            # scores = output[\"scores\"]  \n            \n            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n            results.append(generated_text)\n\n            # # Сохраняем вероятности для сгенерированных токенов\n            # for step, (logits, token_id) in enumerate(zip(scores, generated_ids[0])):\n            #     token_probs = torch.softmax(logits[0], dim=-1).cpu().numpy() # Вероятности для всех токенов\n            #     token_distribution = {\n            #         token_id: {tokenizer.decode([token_id]): prob}\n            #         for token_id, prob in enumerate(token_probs)\n            #     }\n            #     tkn_probs.append({\n            #         \"token\": tokenizer.decode([token_id.item()]),\n            #         \"probability\": token_probs[token_id].item()\n            #     })\n            #     if i == 0:\n            #         # Токен ID и его вероятность\n            #         token_id = generated_ids[0, step].item()\n                    \n            #         token_prob = token_probs[token_id].item()\n                    \n            #         # print(step, tokenizer.decode(token_id), token_probs.max())\n            #         # Сохраняем токен, его вероятность и всё распределение\n            #         tkn_distr_probs.append({\n            #             \"step\": step,\n            #             \"token_id\": token_id,\n            #             \"token\": tokenizer.decode([token_id]),\n            #             \"probability\": token_prob,\n            #             \"distribution\": token_distribution  # Сопоставление токенов и вероятностей\n                    # })\n\n            # token_probabilities[i] = tkn_distr_probs \n            # if len(tkn_probs) != len(scores):\n            #     print(len(tkn_probs), len(scores))\n\n            # log_probs_for_generated = [\n            #     np.log(item[\"probability\"])\n            #     for item in tkn_probs\n            #     if item[\"probability\"] > 0\n            # ]\n\n            # if len(log_probs_for_generated) > 0:  \n            #     total_log_prob = sum(log_probs_for_generated)\n            #     total_words = len(log_probs_for_generated)\n\n            # if total_words > 0:\n            #     perplexities1.append(np.exp(-total_log_prob / total_words))\n            \n    return perplexities1, token_probabilities, results\n\n\nppl1, token_probs, results = calculate_perplexity_and_generated_probabilities(model_new, tokenizer_new, test_data, \n                                               num_samples=num_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T12:14:37.803680Z","iopub.execute_input":"2025-09-10T12:14:37.803968Z","iopub.status.idle":"2025-09-10T13:15:54.271975Z","shell.execute_reply.started":"2025-09-10T12:14:37.803947Z","shell.execute_reply":"2025-09-10T13:15:54.271246Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [1:01:16<00:00,  3.68s/it]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"result_data = pd.DataFrame({'description': test_data1['description'],\n                            'result' : results, 'target' : target})\nresult_data.to_excel('g0_data_gen_qwen1.5_kl-sft.xlsx', index=False)\nresult_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T13:16:12.397118Z","iopub.execute_input":"2025-09-10T13:16:12.397835Z","iopub.status.idle":"2025-09-10T13:16:12.654527Z","shell.execute_reply.started":"2025-09-10T13:16:12.397809Z","shell.execute_reply":"2025-09-10T13:16:12.653921Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                           description  \\\n0    П 20 (№7053/22 1). Пунктирован коллоидный зоб ...   \n1    Пер 11 материал представлен содержимым кистозн...   \n2    В препарате на фоне кистозно-геморрагического ...   \n3    л3 В мазках очень высокой клеточности обнаруже...   \n4    п5 в мазке с гетерогенным клеточным составом н...   \n..                                                 ...   \n995  П17; Л17: в препаратах на фоне коллоидаопредел...   \n996  пунктирован узел коллоидного в разной степени ...   \n997  П4 Материал малоклеточныйпредставлен содержимы...   \n998  материал недостаточно информативный : в мазках...   \n999  П 4 (№6603/21). На фоне элементов хронического...   \n\n                                                result  \\\n0     Доброкачественные образования обеих долей в с...   \n1    В соответствии с критериями   системы классифи...   \n2    Доброкачественное образование в соответствии с...   \n3    В соответствии с критериями  классификационной...   \n4    В соответствии с  классификационной  системой ...   \n..                                                 ...   \n995  В соответствии с критериями Bethesda: доброкач...   \n996                            Bethesda - категория II   \n997  В соответствии с критериями системы классифика...   \n998                             Bethesda - категория I   \n999  Доброкачественное образование в соответствии с...   \n\n                                                target  \n0     Доброкачественные образования обеих долей в с...  \n1    В соответствии с критериями   системы классифи...  \n2    Доброкачественное образование в соответствии с...  \n3    В соответствии с критериями  классификационной...  \n4    В соответствии с  классификационной  системой ...  \n..                                                 ...  \n995  В соответствии с критериями Bethesda: доброкач...  \n996                            Bethesda - категория II  \n997  В соответствии с  классификационной  системой ...  \n998                             Bethesda - категория I  \n999  Доброкачественное образование в соответствии с...  \n\n[1000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>result</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>П 20 (№7053/22 1). Пунктирован коллоидный зоб ...</td>\n      <td>Доброкачественные образования обеих долей в с...</td>\n      <td>Доброкачественные образования обеих долей в с...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Пер 11 материал представлен содержимым кистозн...</td>\n      <td>В соответствии с критериями   системы классифи...</td>\n      <td>В соответствии с критериями   системы классифи...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>В препарате на фоне кистозно-геморрагического ...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>л3 В мазках очень высокой клеточности обнаруже...</td>\n      <td>В соответствии с критериями  классификационной...</td>\n      <td>В соответствии с критериями  классификационной...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>п5 в мазке с гетерогенным клеточным составом н...</td>\n      <td>В соответствии с  классификационной  системой ...</td>\n      <td>В соответствии с  классификационной  системой ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>П17; Л17: в препаратах на фоне коллоидаопредел...</td>\n      <td>В соответствии с критериями Bethesda: доброкач...</td>\n      <td>В соответствии с критериями Bethesda: доброкач...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>пунктирован узел коллоидного в разной степени ...</td>\n      <td>Bethesda - категория II</td>\n      <td>Bethesda - категория II</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>П4 Материал малоклеточныйпредставлен содержимы...</td>\n      <td>В соответствии с критериями системы классифика...</td>\n      <td>В соответствии с  классификационной  системой ...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>материал недостаточно информативный : в мазках...</td>\n      <td>Bethesda - категория I</td>\n      <td>Bethesda - категория I</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>П 4 (№6603/21). На фоне элементов хронического...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n      <td>Доброкачественное образование в соответствии с...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_data1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T11:34:30.242156Z","iopub.execute_input":"2025-08-27T11:34:30.242458Z","iopub.status.idle":"2025-08-27T11:34:30.266930Z","shell.execute_reply.started":"2025-08-27T11:34:30.242410Z","shell.execute_reply":"2025-08-27T11:34:30.266035Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0                                             target  \\\n0        11000  соответствии критериями системы образование ди...   \n1        11001  В соответствии с классификационной системой Be...   \n2        11002  В соответствии с критериями   системы классифи...   \n3        11003  В соответствии с критериями системы классифика...   \n4        11004  В соответствии с критериями системы классифика...   \n..         ...                                                ...   \n95       11095  Цитограмма новообразования из фолликулярных кл...   \n96       11096   Доброкачественные образования в соответствии ...   \n97       11097   Доброкачественные образования обеих долей в с...   \n98       11098  В соответствии с критериями Bethesda: доброкач...   \n99       11099  Неинформативное исследование в соответствии с ...   \n\n                                          description  \n0   п9 В мазке цитологическая картина характерная ...  \n1   Л 3 В мазках неравномерной толщины с очень бол...  \n2   л6 материал представлен кистозно-геморрагическ...  \n3   п13 В мазке неравномерной толщины с обильным п...  \n4   Л1 материал с большой примесью периферической ...  \n..                                                ...  \n95  п22 в мазках на фоне кистозно-геморрагических ...  \n96  П 32 (№1592/22 1). Мазок интенсивно разбавлен ...  \n97  П 6 (№5537/20 1). Мазок интенсивно разбавлен п...  \n98  Л23  В мазке на фоне коллоида эритроцитовмакро...  \n99  Л 17 (№4934/21). В мкудном мазке обнаружено не...  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>target</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11000</td>\n      <td>соответствии критериями системы образование ди...</td>\n      <td>п9 В мазке цитологическая картина характерная ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11001</td>\n      <td>В соответствии с классификационной системой Be...</td>\n      <td>Л 3 В мазках неравномерной толщины с очень бол...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11002</td>\n      <td>В соответствии с критериями   системы классифи...</td>\n      <td>л6 материал представлен кистозно-геморрагическ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11003</td>\n      <td>В соответствии с критериями системы классифика...</td>\n      <td>п13 В мазке неравномерной толщины с обильным п...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11004</td>\n      <td>В соответствии с критериями системы классифика...</td>\n      <td>Л1 материал с большой примесью периферической ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>11095</td>\n      <td>Цитограмма новообразования из фолликулярных кл...</td>\n      <td>п22 в мазках на фоне кистозно-геморрагических ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>11096</td>\n      <td>Доброкачественные образования в соответствии ...</td>\n      <td>П 32 (№1592/22 1). Мазок интенсивно разбавлен ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>11097</td>\n      <td>Доброкачественные образования обеих долей в с...</td>\n      <td>П 6 (№5537/20 1). Мазок интенсивно разбавлен п...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>11098</td>\n      <td>В соответствии с критериями Bethesda: доброкач...</td>\n      <td>Л23  В мазке на фоне коллоида эритроцитовмакро...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>11099</td>\n      <td>Неинформативное исследование в соответствии с ...</td>\n      <td>Л 17 (№4934/21). В мкудном мазке обнаружено не...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom unsloth import FastLanguageModel\nfrom tqdm import tqdm\nimport numpy as np\n\n\n# Функция для вычисления перплексии и вероятностей для сгенерированного текста\ndef calculate_perplexity_and_generated_probabilities(model, tokenizer, data):\n    alpaca_prompt = \"\"\"\n    ### Input:\n    {}\n    \n    ### Output:\n    {}\"\"\"\n    \n    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n    \n    def prompts_func(examples):\n        inputs = examples[\"description\"]\n        texts = []\n        for input in inputs:\n            output = ''\n            text = alpaca_prompt.format(input, output)\n            texts.append(text)\n        return { \"text\" : texts, }\n    \n    \n    X_test = data[['description', 'target']]\n    text_data = prompts_func(X_test)\n    test_data = pd.DataFrame(data=text_data)\n    target = X_test['target'].to_list()\n    \n    num_samples = test_data.shape[0]\n    \n    results = []\n    perplexities1 = []\n    perplexities2 = []\n    token_probabilities = {}\n    model.eval()  # Переводим модель в режим оценки\n    total_log_prob = 0.0\n    total_words = 0\n    with torch.no_grad():\n        for i in tqdm(range(num_samples)):\n            tkn_probs = []\n            tkn_distr_probs = []\n            # Получаем токенизированный текст\n            inputs = tokenizer(test_data['text'][i], return_tensors='pt', truncation=True, max_length=1024).to('cuda')\n\n            # Генерация текста с возвратом вероятностей\n            output = model.generate(\n                **inputs,\n                max_new_tokens=64,\n                use_cache=False,\n                # output_scores=True,\n                pad_token_id=tokenizer.eos_token_id  # Убедитесь, что pad_token_id установлен\n            )\n            \n            # generated_ids = output[\"sequences\"][:, inputs.input_ids.shape[-1]:]  # Только сгенерированные токены\n            # scores = output[\"scores\"]  \n            generated_ids = output[:, inputs['input_ids'].shape[-1]:]\n            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n            results.append(generated_text)\n            # print(generated_text)\n    \n    return results, target\n\n\n\ntest_data1 = pd.read_excel('/kaggle/input/dataf11/data 11.xlsx')[:100]\n\nmax_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# model_name = \"unsloth/Qwen2-1.5B-bnb-4bit\"\n# model_name = \"DimaSK1/Qwen2-1.5B-bnb-4bit_1\"\nfor i in range(6):\n    model_name = f'DimaSK1/Qwen2-1.5B-bnb-4bit_ema_{i}'\n    if i == 0:\n        model_name = 'DimaSK1/Qwen2-1.5B-bnb-4bit_base_ema'\n        \n    model_new, tokenizer_new = FastLanguageModel.from_pretrained(\n        model_name = model_name, # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        load_in_4bit = load_in_4bit,\n    ) # Enable native 2x faster inference\n    \n    FastLanguageModel.for_inference(model_new)\n    \n    results, target = calculate_perplexity_and_generated_probabilities(model_new, tokenizer_new, test_data1)\n    \n    result_data = pd.DataFrame({'description': test_data1['description'],\n                            'result' : results, 'target' : test_data1['target']})\n    result_data.to_excel(f'g{i}_data_test_qwen0.5_sft.xlsx', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T06:37:34.356325Z","iopub.execute_input":"2025-09-05T06:37:34.357231Z","iopub.status.idle":"2025-09-05T06:37:37.599501Z","shell.execute_reply.started":"2025-09-05T06:37:34.357123Z","shell.execute_reply":"2025-09-05T06:37:37.597869Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3360571943.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_device_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."],"ename":"NotImplementedError","evalue":"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# Получение метрик","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade transformers accelerate peft unsloth\n!pip install rouge-score -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import mean_absolute_error\n\n\n# Функция для вычисления BLEU\ndef calculate_bleu(reference, candidate):\n    reference_tokens = [reference.split()]\n    candidate_tokens = candidate.split()\n    return sentence_bleu(reference_tokens, candidate_tokens)\n\n# Функция для вычисления ROUGE\ndef calculate_rouge(reference, candidate):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, candidate)\n    return scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure\n\n# Функция для вычисления BLEU с использованием токенайзера\ndef calculate_bleu_with_tokenizer(candidate, reference, tokenizer):\n    candidate_tokens = tokenizer.tokenize(candidate)\n    reference_tokens = [tokenizer.tokenize(reference)]  # Ожидается список списков для BLEU\n    return sentence_bleu(reference_tokens, candidate_tokens)\n\n# Функция для вычисления ROUGE с использованием токенайзера\ndef calculate_rouge_with_tokenizer(candidate, reference, tokenizer):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    candidate_decoded = tokenizer.decode(tokenizer(candidate)[\"input_ids\"], skip_special_tokens=True)\n    reference_decoded = tokenizer.decode(tokenizer(reference)[\"input_ids\"], skip_special_tokens=True)\n    scores = scorer.score(reference_decoded, candidate_decoded)\n    return scores\n\n# Загружаем русский автотокенайзер\ntokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n\ndf = pd.read_excel('/kaggle/input/g1-gen-qwen-klsft-opt1/g1_data_gen_qwen-klsft_opt.xlsx')[['result', 'target']]\n# df = result_data\n\n# Итерация по строкам датафрейма и вычисление метрик\nresults = []\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    candidate = row['result']\n    reference = row['target']\n    \n    bleu_score = calculate_bleu_with_tokenizer(candidate, reference, tokenizer)\n    rouge_scores = calculate_rouge_with_tokenizer(candidate, reference, tokenizer)\n\n    bleu_notoken = calculate_bleu(reference, candidate)\n    rouge_notoken1, rouge_notoken2, rouge_notokenL = calculate_rouge(reference, candidate)\n    \n    rouge1 = rouge_scores['rouge1'].fmeasure\n    rouge2 = rouge_scores['rouge2'].fmeasure\n    rougeL = rouge_scores['rougeL'].fmeasure\n    \n    results.append({\n        \"BLEU\": bleu_score,\n        \"BLEU_NOTOKEN\": bleu_notoken,\n        \"ROUGE-1\": rouge1,\n        \"ROUGE1_NOTOKEN\": rouge_notoken1,\n        \"ROUGE-2\": rouge2,\n        \"ROUGE2_NOTOKEN\": rouge_notoken2,\n        \"ROUGE-L\": rougeL,\n        \"ROUGEL_NOTOKEN\": rouge_notokenL\n    })\n\n# Создаем датафрейм с результатами\nresults_df = pd.DataFrame(results)\n# print(results_df)\nresults_df.to_excel('Метрики_qwen_klsft_0_opt.xlsx', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BLUERT, METEOR и другие","metadata":{}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/google-research/bleurt.git\n%cd bleurt\n!pip install .\n!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n!unzip BLEURT-20.zip\n!pip install nltk sacrebleu moverscore bert-score --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.translate.meteor_score import meteor_score\nfrom sacrebleu.metrics import CHRF\nfrom bleurt import score as bleurt_score\n# from moverscore_v2 import get_idf_dict, word_mover_score\nimport bert_score\nfrom tqdm import tqdm\nimport re\n\n\n# Загружаем токенизатор для NLTK\nnltk.download('punkt')\nnltk.download('wordnet')\n\n# Инициализация метрик\nchrf = CHRF()\nbleurt = bleurt_score.BleurtScorer(\"/content/bleurt/BLEURT-20\")   # скачай модель заранее: https://github.com/google-research/bleurt\n\n# ==== ФУНКЦИИ ====\ndef calculate_metrics(reference, candidate):\n    \"\"\"Вычисляем все метрики для пары текстов\"\"\"\n\n    # METEOR\n    meteor = meteor_score([reference.split()], candidate.split())\n\n    # ChrF\n    chrf_score = chrf.sentence_score(candidate, [reference]).score\n\n    # BLEURT\n    bleurt_val = bleurt.score(references=[reference], candidates=[candidate])[0]\n\n    # MoverScore\n    # idf_dict_ref = get_idf_dict([reference])\n    # idf_dict_cand = get_idf_dict([candidate])\n    # mover = word_mover_score([reference], [candidate],\n    #                          idf_dict_ref, idf_dict_cand,\n    #                          stop_words=[], n_gram=1, remove_subwords=True)[0]\n\n    # BERTScore (используем модель для русского — 'xlm-roberta-large')\n    P, R, F1 = bert_score.score([candidate], [reference], lang=\"ru\", model_type=\"xlm-roberta-large\")\n    bert_f1 = F1.mean().item()\n\n    return {\n        \"METEOR\": meteor,\n        \"ChrF\": chrf_score,\n        \"BLEURT\": bleurt_val,\n        # \"MoverScore\": mover,\n        \"BERTScore_F1\": bert_f1\n    }\n\n# ==== НОРМАЛИЗАЦИЯ ====\ndef normalize_text(text: str) -> str:\n    \"\"\"Приведение текста к единому виду\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower().strip()\n    text = re.sub(r\"\\s+\", \" \", text)   # убираем лишние пробелы\n    text = re.sub(r\"[^\\w\\sа-яё]\", \"\", text)  # оставляем только буквы и цифры\n    return text\n\n\ndef evaluate_file(filepath, output_path):\n    \"\"\"Считает метрики для Excel-файла с колонками result/target\"\"\"\n    df = pd.read_excel(filepath)[[\"result\", \"target\"]]\n\n    # нормализация\n    df[\"result\"] = df[\"result\"].apply(normalize_text)\n    df[\"target\"] = df[\"target\"].apply(normalize_text)\n\n    results = []\n    print(f'Обработка {filepath}...')\n    for i in tqdm(range(df.shape[0])):\n      row = df.iloc[i]\n\n      cand = str(row.iloc[0])\n      ref = str(row.iloc[1])\n      scores = calculate_metrics(ref, cand)\n      results.append(scores)\n\n    results_df = pd.DataFrame(results)\n\n    # Сохраняем только построчные метрики, без среднего\n    results_df.to_excel(output_path, index=False)\n    print(f\"[+] Файл сохранён: {output_path}\")\n\n# ==== ПРИМЕР ЗАПУСКА ====\nfiles = [f\"/content/bleurt/test_data/g{i}_data_test_qwen_tema.xlsx\" for i in range(4, 8)]\nfor i, f in enumerate(files):\n    evaluate_file(f, f\"Метрики_1_qwen_test_tema_{i}.xlsx\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Визуализация метрик","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Ввод параметров\nmethods = ['tema', 'sft', 'klsft']\nnum_generations = 7\ntrh = 0.9\n\nprint(f\"Анализируемые методы: {methods}\")\nprint(f\"Количество поколений: {num_generations}\")\nprint(f\"Пороговое значение: {trh}\")\n\n# Инициализация структур данных\nmetrics = [\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\ndata_values = {metric: {method: [] for method in methods} for metric in metrics}\ndata_stds = {metric: {method: [] for method in methods} for metric in metrics}\n\n# Цикл по методам и поколениям\nfor method in methods:\n    for gen in range(num_generations):\n        file_path = Path(f'Метрики_{method}_{gen}.xlsx')\n        \n        if file_path.exists():\n            try:\n                df = pd.read_excel(file_path, engine='openpyxl')\n                # print(f\"✅ Загружен файл: {file_path}\")\n                \n                # Обработка каждой метрики\n                for metric in metrics:\n                    # Поиск правильного названия столбца\n                    col_name = None\n                    for variant in [metric.upper(), metric, metric.capitalize()]:\n                        if variant in df.columns:\n                            col_name = variant\n                            break\n                    \n                    if col_name is None:\n                        print(f\"⚠️ Столбец '{metric}' не найден в {file_path}\")\n                        data_values[metric][method].append(0)\n                        data_stds[metric][method].append(0)\n                        continue\n                    \n                    # Расчет метрик\n                    values = df[col_name]\n                    above_threshold = values[values > trh].count() / len(values)\n                    std_dev = values.std()\n                    \n                    data_values[metric][method].append(above_threshold)\n                    data_stds[metric][method].append(std_dev)\n            \n            except Exception as e:\n                print(f\"⛔ Ошибка при обработке {file_path}: {str(e)}\")\n                for metric in metrics:\n                    data_values[metric][method].append(0)\n                    data_stds[metric][method].append(0)\n        else:\n            print(f\"⚠️ Файл не найден: {file_path}\")\n            for metric in metrics:\n                data_values[metric][method].append(0)\n                data_stds[metric][method].append(0)\n\n# Настройки визуализации\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(14, 8))\n\n# Цвета для методов\nmethod_colors = plt.cm.tab10(np.linspace(0, 1, len(methods)))\n\n# Маркеры для метрик\nmarkers = ['o', 's', 'D', '^']\nmetric_markers = {metric: marker for metric, marker in zip(metrics, markers)}\n\n# Генерация оси X (поколения)\nx = np.arange(num_generations)\n\n# Построение графиков\nfor method_idx, method in enumerate(methods):\n    color = method_colors[method_idx]\n    \n    for metric in metrics:\n        y = data_values[metric][method]\n        y_err = data_stds[metric][method]\n        marker = metric_markers[metric]\n        \n        # Линия для метода (один раз для каждой метрики)\n        plt.plot(x, y, \n                 color=color, \n                 linestyle='-', \n                 alpha=0.7, \n                 zorder=1)\n        \n        # Точки с ошибками\n        plt.errorbar(x, y, yerr=y_err,\n                     fmt=marker,\n                     color=color,\n                     ecolor=color,\n                     markersize=10,\n                     capsize=4,\n                     capthick=1.5,\n                     elinewidth=1.5,\n                     label=f'{method} - {metric}' if method_idx == 0 else \"\",\n                     zorder=2)\n\n# Настройка осей и заголовка\nplt.xticks(x, [f'Поколение {i}' for i in range(num_generations)], fontsize=12)\nplt.yticks(np.linspace(0, 1.0, 11), fontsize=10)\nplt.ylabel(\"Доля значений > порога\", fontsize=14)\nplt.xlabel(\"Номер поколения\", fontsize=14)\nplt.title(f\"Сравнение метрик по поколениям (порог = {trh})\", fontsize=16, fontweight='bold')\nplt.ylim(-0.05, 1.05)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Создание кастомной легенды\nlegend_elements = []\n\n# Добавление элементов для методов (цветные линии)\nfor method_idx, method in enumerate(methods):\n    legend_elements.append(Line2D([0], [0], \n                                 color=method_colors[method_idx], \n                                 lw=3,\n                                 label=method))\n\n# Добавление элементов для метрик (маркеры)\nfor metric, marker in metric_markers.items():\n    legend_elements.append(Line2D([0], [0], \n                                 marker=marker, \n                                 color='w', \n                                 markerfacecolor='black',\n                                 markersize=10,\n                                 label=metric.upper()))\n\n# Размещение легенды\nplt.legend(handles=legend_elements, \n           loc='upper center', \n           bbox_to_anchor=(0.5, -0.12),\n           ncol=len(methods) + 1,  # +1 для метрик\n           fontsize=10,\n           frameon=True,\n           shadow=True)\n\n# Сохранение и отображение\nplt.tight_layout()\nfilename = f'metrics_comparison_{trh}.jpeg'\nplt.savefig(filename, dpi=300, bbox_inches='tight')\nprint(f\"✅ График сохранен как: {filename}\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Оценка модели на бенчмарке MMLU","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade transformers accelerate peft unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T14:21:51.071397Z","iopub.execute_input":"2025-09-21T14:21:51.072292Z","iopub.status.idle":"2025-09-21T14:26:52.061469Z","shell.execute_reply.started":"2025-09-21T14:21:51.072258Z","shell.execute_reply":"2025-09-21T14:26:52.060364Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport random\nimport json\nimport dataclasses\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Any\nfrom tqdm import tqdm\n\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom unsloth import FastLanguageModel\n\n# === Конфигурация по умолчанию ===\nMEDICAL_SUBJECTS = ['all']\n# MEDICAL_SUBJECTS = [\n#     \"anatomy\",\n#     \"clinical_knowledge\",\n#     \"college_biology\",\n#     # \"college_medicine\",\n#     \"medical_genetics\",\n#     \"nutrition\",\n#     # \"professional_medicine\",\n#     \"virology\",\n#     \"philosophy\",\n#     \"formal_logic\",\n#     \"global_facts\",\n#     \"public_relations\"\n# ]\n# 'abstract_algebra', 'all', 'anatomy', 'astronomy', 'auxiliary_train', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n\nUSER_TPL = (\n    \"Вопрос: {question}\\n\"\n    \"Варианты ответа:\\n\"\n    \"A) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\\n\"\n    \"Ответ:\"\n)\n\nFEW_SHOT_TPL = (\n    \"Вопрос: {question}\\n\"\n    \"Варианты ответа:\\n\"\n    \"A) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\"\n    \"Правильный ответ: {answer}\\n\\n\"\n)\n\nLETTER_TO_INDEX = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\nINDEX_TO_LETTER = {v: k for k, v in LETTER_TO_INDEX.items()}\n\n# === Вспомогательные функции ===\ndef normalize_answer_field(example: Dict[str, Any]) -> Dict[str, Any]:\n    q = example.get(\"question\") or example.get(\"input\") or example.get(\"prompt\") or \"\"\n    choices = example.get(\"choices\") or example.get(\"options\")\n    # print(choices)\n    if not choices:\n        candidates = [example.get(k) for k in [\"A\", \"B\", \"C\", \"D\"]]\n        if all(isinstance(x, str) and x for x in candidates):\n            choices = candidates\n    assert choices and len(choices) == 4, f\"Некорректные варианты: {choices}\"\n\n    ans = example.get(\"answer\") or example.get(\"target\")\n    # print(ans)\n    if isinstance(ans, str):\n        ans = ans.strip()\n        if ans in LETTER_TO_INDEX:\n            gold_idx = LETTER_TO_INDEX[ans]\n        else:\n            try:\n                gold_idx = choices.index(ans)\n            except ValueError:\n                gold_idx = None\n    elif isinstance(ans, int):\n        gold_idx = ans\n    elif ans is None:\n        gold_idx = 0\n    assert gold_idx in [0, 1, 2, 3], f\"Некорректный ответ: {ans}, {q}\"\n    return {\"question\": q, \"choices\": choices, \"gold\": gold_idx}\n\n\ndef build_few_shot_block(few_shot_examples: List[Dict[str, Any]]) -> str:\n    return \"\".join(\n        FEW_SHOT_TPL.format(\n            question=ex[\"question\"], A=ex[\"choices\"][0], B=ex[\"choices\"][1],\n            C=ex[\"choices\"][2], D=ex[\"choices\"][3], answer=INDEX_TO_LETTER[ex[\"gold\"]]\n        )\n        for ex in few_shot_examples\n    )\n\n\ndef build_prompt(example: Dict[str, Any], few_shot_block: str = \"\") -> str:\n    A, B, C, D = example[\"choices\"]\n    user = USER_TPL.format(question=example[\"question\"], A=A, B=B, C=C, D=D)\n    if few_shot_block:\n        return (\"Ты увидешь несколько решенных вопросов с правильным ответом.\\n\\n\"\n                + few_shot_block\n                + \"Далее ответь на следующий вопрос.\\n\\n\"\n                + user)\n    return user\n\n\ndef extract_letter(text: str) -> Optional[str]:\n    if not text:\n        return None\n    m = re.search(r\"\\b([A-D])\\b\", text.strip())\n    if m:\n        return m.group(1)\n    m = re.search(r\"([A-D])\\)?\\s*$\", text.strip())\n    return m.group(1) if m else None\n\n# === Генерация с моделью Unsloth ===\ndef generate(prompt: str, max_new_tokens: int = 128) -> str:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, use_cache=False)\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text[len(prompt):]\n\n@dataclass\nclass EvalConfig:\n    subjects: List[str] = dataclasses.field(default_factory=lambda: MEDICAL_SUBJECTS)\n    k_shot: int = 5\n    limit_per_subject: Optional[int] = None\n    seed: int = 42\n    sleep: float = 0.0\n\n@dataclass\nclass EvalResult:\n    subject: str\n    n: int\n    correct: int\n    accuracy: float\n    eval_time: float\n\ndef load_subject(subject: str) -> Dataset:\n    return load_dataset(\"cais/mmlu\", subject)\n\ndef pick_few_shots(ds_valid: Optional[Dataset], k: int, rng: random.Random) -> List[Dict[str, Any]]:\n    if not ds_valid or len(ds_valid) == 0 or k <= 0:\n        return []\n    idxs = list(range(len(ds_valid)))\n    rng.shuffle(idxs)\n    return [normalize_answer_field(ds_valid[i]) for i in idxs[:k]]\n\ndef evaluate_subject(subject: str, cfg: EvalConfig, rng: random.Random) -> EvalResult:\n\n    raw = load_subject(subject)\n    test_split = raw.get(\"test\", 0)\n    valid_split = raw.get(\"validation\", 0) or raw.get(\"dev\", 0)\n    if test_split == 0 or valid_split == 0:\n        return None\n\n    few_shot_block = build_few_shot_block(pick_few_shots(valid_split, cfg.k_shot, rng))\n\n    n_total, n_correct = 0, 0\n    records = []\n    start_time = time.time()\n    if cfg.limit_per_subject is not None:\n        lim = min(cfg.limit_per_subject, len(test_split))\n    else:\n        lim = len(test_split)\n    for i in tqdm(range(lim)):\n        ex = normalize_answer_field(test_split[i])\n        prompt = build_prompt(ex, few_shot_block)\n        print(prompt)\n        raw_out = generate(prompt)\n        pred_letter = extract_letter(raw_out) or \"\"\n        pred_idx = LETTER_TO_INDEX.get(pred_letter, -1)\n        correct = int(pred_idx == ex[\"gold\"])\n        n_total += 1\n        n_correct += correct\n        records.append({\n            \"subject\": subject,\n            \"index\": i,\n            \"question\": ex[\"question\"],\n            \"A\": ex[\"choices\"][0], \"B\": ex[\"choices\"][1],\n            \"C\": ex[\"choices\"][2], \"D\": ex[\"choices\"][3],\n            \"gold_letter\": INDEX_TO_LETTER[ex[\"gold\"]],\n            \"prediction_raw\": raw_out.strip(),\n            \"prediction_letter\": pred_letter,\n            \"correct\": correct\n        })\n        if cfg.sleep:\n            time.sleep(cfg.sleep)\n    end_time = time.time()\n\n    eval_time = end_time - start_time\n    acc = n_correct / max(1, n_total)\n\n    # os.makedirs(\"/kaggle/working/mmlu_med_logs\", exist_ok=True)\n    # import pandas as pd\n    # pd.DataFrame.from_records(records).to_excel(f\"/kaggle/working/mmlu_med_logs/{model_number}_{subject}.xlsx\", index=False)\n    \n    return EvalResult(subject, n_total, n_correct, acc, eval_time)\n\ndef run(subjects: Optional[List[str]] = None, k_shot: int = 5, limit_per_subject: Optional[int] = None, seed: int = 42, sleep: float = 0.0):\n    cfg = EvalConfig(subjects=subjects or MEDICAL_SUBJECTS, k_shot=k_shot, limit_per_subject=limit_per_subject, seed=seed, sleep=sleep)\n    rng = random.Random(cfg.seed)\n\n    results: List[EvalResult] = []\n    for s in cfg.subjects:\n        print(f\"\\n=== Evaluating subject: {s} ===\")\n        res = evaluate_subject(s, cfg, rng)\n        if res is None:\n            continue\n        print(f\"{s}: n={res.n}, acc={res.accuracy:.4f}\")\n        results.append(res)\n\n    total_n = sum(r.n for r in results)\n    total_correct = sum(r.correct for r in results)\n    micro_acc = total_correct / max(1, total_n)\n    macro_acc = sum(r.accuracy for r in results) / max(1, len(results))\n\n    print(\"\\n===== SUMMARY (medical MMLU) =====\")\n    for r in results:\n        print(f\"{r.subject:24s}  acc={r.accuracy:.4f}  n={r.n}\")\n    print(\"---------------------------------------------\")\n    print(f\"Micro-avg accuracy: {micro_acc:.4f}\")\n    # print(f\"Macro-avg accuracy: {macro_acc:.4f}\")\n\n    import pandas as pd\n    df = pd.DataFrame([{ \"subject\": r.subject, \"n\": r.n, \"correct\": r.correct, \"accuracy\": r.accuracy, \"eval_time\": r.eval_time } for r in results])\n    df.loc[len(df)] = {\"subject\": \"_TOTAL_micro_\", \"n\": total_n, \"correct\": total_correct, \"accuracy\": micro_acc}\n    df.loc[len(df)] = {\"subject\": \"_MEAN_macro_\", \"n\": len(results), \"correct\": None, \"accuracy\": macro_acc}\n    df.to_excel(f\"/kaggle/working/mmlu_med_summary_{model_number}.xlsx\", index=False)\n    print(f\"\\nSaved summary: /kaggle/working/mmlu_med_summary_{model_number}.xlsx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T14:26:58.449654Z","iopub.execute_input":"2025-09-21T14:26:58.449966Z","iopub.status.idle":"2025-09-21T14:27:44.478599Z","shell.execute_reply.started":"2025-09-21T14:26:58.449935Z","shell.execute_reply":"2025-09-21T14:27:44.477767Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-09-21 14:27:10.147367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758464830.494543      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758464830.592260      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Настройки модели ===\nmax_seq_length = 1024\nload_in_4bit = True\n# model_name = \"unsloth/Qwen2-0.5B-bnb-4bit\"\nmodel_name = \"DimaSK1/Qwen2-0.5B-bnb-4bit-sft_base\"\n# model_name = 'DimaSK1/Qwen2-1.5B-bnb-4bit_base_sft'\nmodel_number = 'qwen0.5-sft'\n# model_name = \"unsloth/mistral-7b-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T14:27:50.944918Z","iopub.execute_input":"2025-09-21T14:27:50.945255Z","iopub.status.idle":"2025-09-21T14:28:06.915020Z","shell.execute_reply.started":"2025-09-21T14:27:50.945232Z","shell.execute_reply":"2025-09-21T14:28:06.914177Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.9.7: Fast Bit patching. Transformers: 4.55.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/457M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91bf271e4f6f47bbb30b542e39e06f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4f50356d24448fbfa2bd76fb716761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53246bab81e4dbcb92cfb7804842cd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253197a1314d4ba5b4ca951bcfce4d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c54a24d9dbd42e481d253979886e7f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb584abf2d646c487e8edca71cf16c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/107 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"096b2f06cc2f4e2398fb471d5c8951c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0af2b47954647629795de99657e3e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/17.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fad514c80a47378932d65e8d3e9235"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"run(k_shot=5, limit_per_subject=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T14:28:06.916274Z","iopub.execute_input":"2025-09-21T14:28:06.916661Z","iopub.status.idle":"2025-09-21T14:28:15.489232Z","shell.execute_reply.started":"2025-09-21T14:28:06.916631Z","shell.execute_reply":"2025-09-21T14:28:15.488292Z"}},"outputs":[{"name":"stdout","text":"\n=== Evaluating subject: all ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c083581bece43cc9caaef7bcc624fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65597e46d3bb4ce0a5a3457b34d21067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b73175456e49929e27159da5a18767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"589601cd7e5242028a364842803ca154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd66e635546945748d02da8560f2f06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba5937d10834a978bd5c049c951cf5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925a9a94f8a5413d8aeb878b708a3585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36b6407c243429f98c99fe1f080cd57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46569fc1d79c47ecbc859e98ce9d2bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006d4b3d9fb2400ab7847f9eedc851df"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ты увидешь несколько решенных вопросов с правильным ответом.\n\nВопрос: A 60-year-old man has had painful skin with exfoliation of the skin and mucous membranes for 1 day. He has been taking allopurinol and probenecid for 2 weeks because of gouty arthritis. There is diffuse exfoliation of the skin with oozing of serous fluid. The mucous membranes of the mouth are erythematous and exfoliated. There are no target lesions. Which of the following is the most likely diagnosis?\nВарианты ответа:\nA) Erythema multiforme\nB) Pemphigus erythematosus\nC) Staphylococcal scalded-skin syndrome\nD) Toxic epidermal necrolysis\nПравильный ответ: D\n\nВопрос: A local library has a scanner to detect library materials that have not been checked out. Each item has a chip somewhere inside. Upon checkout, the chip is deactivated so the scanner will not set off the alarm. The scanner has a 98% chance of detecting an active chip (meaning the material has not been checked out) and setting off the alarm. The scanner also has a 3% chance of sounding the alarm when someone passes through without an active chip. It is estimated that 0.5% of library customers actually try to leave the library with an active chip. What is the probability that, if the alarm sounds, the patron leaving the library has an item with an active chip?\nВарианты ответа:\nA) 0.0049\nB) 0.0348\nC) 0.141\nD) 0.97\nПравильный ответ: C\n\nВопрос:  According to Mill, the only things desirable as ends are\nВарианты ответа:\nA) virtuous character traits.\nB) satisfactions of prima facie duties.\nC) pleasure and freedom from pain.\nD) satisfactions of the categorical imperative.\nПравильный ответ: C\n\nВопрос: Consider the following reaction showing photosynthesis: 6CO2(g) + 6H2O(l) → C6H12O6(s) + 6O2(g) DH = + 2800 kJ/mol Which of the following is true regarding the thermal energy in this system?\nВарианты ответа:\nA) It is transferred from the surroundings to the reaction.\nB) It is transferred from the reaction to the surroundings.\nC) It is transferred from the reactants to the products.\nD) It is transferred from the products to the reactants.\nПравильный ответ: A\n\nВопрос: On the TV show 'Hill Street Blues' What is Joyce Davenport's nickname For Captain Frank Furillo?\nВарианты ответа:\nA) Furry Monster\nB) Pizza Man\nC) Snookums\nD) Baby Cakes\nПравильный ответ: B\n\nДалее ответь на следующий вопрос.\n\nВопрос: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\nВарианты ответа:\nA) 0\nB) 4\nC) 2\nD) 6\n\nОтвет:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"all: n=1, acc=0.0000\n\n===== SUMMARY (medical MMLU) =====\nall                       acc=0.0000  n=1\n---------------------------------------------\nMicro-avg accuracy: 0.0000\n\nSaved summary: /kaggle/working/mmlu_med_summary_qwen0.5-sft.xlsx\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Визуализация","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# --- Параметры ---\nmethods = ['tema', 'sft', 'kl_sft']\nbase_method = 'base'\nnum_generations = 8\ngen_to_plot = 0\n\n# --- Загрузка данных ---\nall_data = {}\nsubjects = set()\n\n# Загружаем данные для базовой модели\nbase_file_path = Path('mmlu_med_summary_base.xlsx')\nif base_file_path.exists():\n    df_base = pd.read_excel(base_file_path, engine='openpyxl')\n    subj_col = next((c for c in df_base.columns if c.lower() in [\"subject\", \"area\", \"domain\"]), None)\n    acc_col = next((c for c in df_base.columns if \"acc\" in c.lower()), None)\n\n    if subj_col and acc_col:\n        df_base = df_base[[subj_col, acc_col]].rename(columns={subj_col: \"subject\", acc_col: \"accuracy\"})\n        df_base['subject'] = df_base['subject'].replace('_TOTAL_micro_', 'Общая оценка')\n        all_data[base_method] = {gen_to_plot: df_base}\n        subjects.update(df_base[\"subject\"].unique())\n        print(\"✅ Загружены данные базовой модели\")\n\n# Загружаем остальные методы\nfor method in methods:\n    all_data[method] = {}\n    file_path = Path(f'mmlu_med_summary_{method}_{gen_to_plot}.xlsx')\n    if file_path.exists():\n        df = pd.read_excel(file_path, engine='openpyxl')\n        subj_col = next((c for c in df.columns if c.lower() in [\"subject\", \"area\", \"domain\"]), None)\n        acc_col = next((c for c in df.columns if \"acc\" in c.lower()), None)\n        if subj_col and acc_col:\n            df = df[[subj_col, acc_col]].rename(columns={subj_col: \"subject\", acc_col: \"accuracy\"})\n            df['subject'] = df['subject'].replace('_TOTAL_micro_', 'Общая оценка')\n            all_data[method][gen_to_plot] = df\n            subjects.update(df[\"subject\"].unique())\n\nsubjects = sorted(list(subjects))[1:]\n\n# --- Подготовка данных ---\ndata_to_plot = {}\nfor subject in subjects:\n    data_to_plot[subject] = []\n    for method in [base_method] + methods:\n        df = all_data.get(method, {}).get(gen_to_plot)\n        if df is not None:\n            row = df[df[\"subject\"] == subject]\n            data_to_plot[subject].append(row[\"accuracy\"].values[0] if not row.empty else 0)\n        else:\n            data_to_plot[subject].append(0)\n\n# --- Визуализация ---\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nall_methods = [base_method] + methods\ncolors = plt.cm.Paired(np.linspace(0, 1, len(all_methods)))\n\nn_subjects = len(subjects)\nn_methods = len(all_methods)\nbar_width = 0.8 / n_methods\nx = np.arange(n_subjects)\n\nfig, ax = plt.subplots(figsize=(18, 9))\n\n# Рисуем столбцы\nfor i, method in enumerate(all_methods):\n    values = [data_to_plot[sub][i] for sub in subjects]\n    offset = (i - n_methods/2 + 0.5) * bar_width\n    bars = ax.bar(x + offset, values, bar_width, label=method, color=colors[i], edgecolor=\"black\", alpha=0.85)\n\n    # Подписи значений внутри баров\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate(f\"{height:.2f}\",\n                    xy=(bar.get_x() + bar.get_width()/2, height),\n                    xytext=(0, 3),  # смещение\n                    textcoords=\"offset points\",\n                    ha=\"center\", va=\"bottom\",\n                    fontsize=8)\n\n# Настройка графика\nax.set_xlabel(\"Датасеты\", fontsize=14, labelpad=10)\nax.set_ylabel(\"Точность\", fontsize=14, labelpad=10)\nax.set_title(f\"Сравнение моделей на MMLU\", fontsize=16, fontweight=\"bold\")\nax.set_xticks(x)\nax.set_xticklabels(subjects, rotation=40, ha=\"right\", fontsize=11)\nax.legend(title=\"Методы\", fontsize=12, title_fontsize=12, frameon=True, shadow=True)\nax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.6)\n\nplt.tight_layout()\nplt.savefig(f\"barplot_medical_gen_{gen_to_plot}.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Оценка модели на бенчмарке MedMCQA","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q --upgrade transformers accelerate peft unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:21:12.094304Z","iopub.execute_input":"2025-08-21T13:21:12.094959Z","iopub.status.idle":"2025-08-21T13:25:49.579146Z","shell.execute_reply.started":"2025-08-21T13:21:12.094935Z","shell.execute_reply":"2025-08-21T13:25:49.578094Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport random\nimport dataclasses\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Any\nfrom tqdm import tqdm\n\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom unsloth import FastLanguageModel\n\n# === Конфигурация ===\nUSER_TPL = (\n    \"Вопрос: {question}\\n\"\n    \"Варианты ответа:\\n\"\n    \"A) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\\n\"\n    \"Ответ:\"\n)\n\nFEW_SHOT_TPL = (\n    \"Вопрос: {question}\\n\"\n    \"Варианты ответа:\\n\"\n    \"A) {A}\\nB) {B}\\nC) {C}\\nD) {D}\\n\"\n    \"Правильный ответ: {answer}\\n\\n\"\n)\n\nLETTER_TO_INDEX = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\nINDEX_TO_LETTER = {v: k for k, v in LETTER_TO_INDEX.items()}\n\n\n# === Вспомогательные функции ===\ndef normalize_answer_field(example: Dict[str, Any]) -> Dict[str, Any]:\n    q = example[\"question\"]\n    choices = [example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]]\n    ans = int(example[\"cop\"])  # gold индекс 0..3\n    subject = example.get(\"subject_name\", \"unknown\")\n    return {\"question\": q, \"choices\": choices, \"gold\": ans, \"subject\": subject}\n\n\ndef build_few_shot_block(few_shot_examples: List[Dict[str, Any]]) -> str:\n    return \"\".join(\n        FEW_SHOT_TPL.format(\n            question=ex[\"question\"], A=ex[\"choices\"][0], B=ex[\"choices\"][1],\n            C=ex[\"choices\"][2], D=ex[\"choices\"][3], answer=INDEX_TO_LETTER[ex[\"gold\"]]\n        )\n        for ex in few_shot_examples\n    )\n\n\ndef build_prompt(example: Dict[str, Any], few_shot_block: str = \"\") -> str:\n    A, B, C, D = example[\"choices\"]\n    user = USER_TPL.format(question=example[\"question\"], A=A, B=B, C=C, D=D)\n    if few_shot_block:\n        return (\"Ты увидишь несколько решенных вопросов с правильным ответом.\\n\\n\"\n                + few_shot_block\n                + \"Далее ответь на следующий вопрос.\\n\\n\"\n                + user)\n    return user\n\n\ndef extract_letter(text: str) -> Optional[str]:\n    if not text:\n        return None\n    m = re.search(r\"\\b([A-Da-d])\\b\", text.strip())\n    if m:\n        return m.group(1)\n    m = re.search(r\"([A-Da-d])\\)?\\s*$\", text.strip())\n    return m.group(1) if m else None\n\n\n# === Генерация с моделью Unsloth ===\ndef generate(prompt: str, max_new_tokens: int = 128) -> str:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text[len(prompt):]\n\n\n@dataclass\nclass EvalConfig:\n    k_shot: int = 5\n    limit_per_subject: Optional[int] = 500  # ограничение на примеры\n    seed: int = 42\n    sleep: float = 0.0\n\n\n@dataclass\nclass EvalResult:\n    subject: str\n    n: int\n    correct: int\n    accuracy: float\n\n\n# === Подготовка датасета MedMCQA ===\ndef load_medmcqa():\n    ds = load_dataset(\"openlifescienceai/medmcqa\")\n    return ds[\"validation\"], ds[\"train\"]\n\n\ndef pick_few_shots(ds_valid, k: int, rng: random.Random) -> List[Dict[str, Any]]:\n    idxs = list(range(len(ds_valid)))\n    rng.shuffle(idxs)\n    return [normalize_answer_field(ds_valid[i]) for i in idxs[:k]]\n\n\ndef evaluate(cfg: EvalConfig, rng: random.Random):\n    valid_split, test_split = load_medmcqa()\n    few_shot_block = build_few_shot_block(pick_few_shots(valid_split, cfg.k_shot, rng))\n\n    records = []\n    subject_results = []\n\n    # === группируем test_split по subject ===\n    df_test = pd.DataFrame([normalize_answer_field(ex) for ex in test_split])\n    subjects = df_test[\"subject\"].unique()\n\n    for subj in subjects:\n        subj_records = []\n        group = df_test[df_test[\"subject\"] == subj]\n        lim = min(cfg.limit_per_subject, len(group))\n        sampled = group.sample(n=lim, random_state=cfg.seed)\n\n        n_total, n_correct = 0, 0\n        print(f'===== SUBJECT: {subj} =====')\n        for i in tqdm(range(sampled.shape[0])):\n            ex = sampled.iloc[i]\n            prompt = build_prompt(ex.to_dict(), few_shot_block)\n            raw_out = generate(prompt)\n            pred_letter = extract_letter(raw_out) or \"\"\n            pred_idx = LETTER_TO_INDEX.get(pred_letter, -1)\n            correct = int(pred_idx == ex[\"gold\"])\n            n_total += 1\n            n_correct += correct\n\n            records.append({\n                \"subject\": subj,\n                \"question\": ex[\"question\"],\n                \"A\": ex[\"choices\"][0], \"B\": ex[\"choices\"][1],\n                \"C\": ex[\"choices\"][2], \"D\": ex[\"choices\"][3],\n                \"gold_letter\": INDEX_TO_LETTER[ex[\"gold\"]],\n                \"prediction_raw\": raw_out.strip(),\n                \"prediction_letter\": pred_letter,\n                \"correct\": correct,\n            })\n            \n            subj_records.append({\n                \"subject\": subj,\n                \"question\": ex[\"question\"],\n                \"A\": ex[\"choices\"][0], \"B\": ex[\"choices\"][1],\n                \"C\": ex[\"choices\"][2], \"D\": ex[\"choices\"][3],\n                \"gold_letter\": INDEX_TO_LETTER[ex[\"gold\"]],\n                \"prediction_raw\": raw_out.strip(),\n                \"prediction_letter\": pred_letter,\n                \"correct\": correct,\n            })\n            \n            if cfg.sleep:\n                time.sleep(cfg.sleep)\n\n        acc = n_correct / max(1, n_total)\n        print(f\"{subj}: n = {n_total:.4f}, acc={acc:.4f}\")\n        subject_results.append(EvalResult(subj, n_total, n_correct, acc))\n        pd.DataFrame.from_records(subj_records).to_excel(f\"/kaggle/working/medmcqa_logs/{model_number}_{subj}.xlsx\", index=False)\n    # === общий результат ===\n    df = pd.DataFrame.from_records(records)\n    n_total = len(df)\n    n_correct = df[\"correct\"].sum()\n    acc_total = n_correct / max(1, n_total)\n    subject_results.append(EvalResult(\"_TOTAL_\", n_total, n_correct, acc_total))\n\n    # === сохранение ===\n    # os.makedirs(\"/kaggle/working/medmcqa_logs\", exist_ok=True)\n    df.to_excel(f\"/kaggle/working/medmcqa_logs/{model_number}_medmcqa.xlsx\", index=False)\n\n    df_summary = pd.DataFrame([dataclasses.asdict(r) for r in subject_results])\n    df_summary.to_excel(f\"/kaggle/working/medmcqa_summary_{model_number}.xlsx\", index=False)\n\n    return subject_results\n\n\ndef run(k_shot: int = 5, limit_per_subject: int = 500, seed: int = 42, sleep: float = 0.0):\n    cfg = EvalConfig(k_shot=k_shot, limit_per_subject=limit_per_subject, seed=seed, sleep=sleep)\n    rng = random.Random(cfg.seed)\n\n    print(f\"\\n=== Evaluating MedMCQA (with subject_name breakdown) ===\")\n    os.makedirs(\"/kaggle/working/medmcqa_logs\", exist_ok=True)\n    results = evaluate(cfg, rng)\n\n    print(\"\\n===== SUMMARY (MedMCQA) =====\")\n    for r in results:\n        print(f\"{r.subject:30s}  acc={r.accuracy:.4f}  n={r.n}\")\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:26:38.541454Z","iopub.execute_input":"2025-08-21T13:26:38.542247Z","iopub.status.idle":"2025-08-21T13:27:22.783773Z","shell.execute_reply.started":"2025-08-21T13:26:38.542214Z","shell.execute_reply":"2025-08-21T13:27:22.783008Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-21 13:26:49.951600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755782810.327034      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755782810.432844      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Настройки модели ===\nmax_seq_length = 1024\nload_in_4bit = True\n# model_name = \"unsloth/Qwen2-1.5B-bnb-4bit\"\nmodel_name = \"DimaSK1/Qwen2-1.5B-bnb-4bit_new_trainer\"\nmodel_number = 'kl_sft_0'\n# model_name = \"unsloth/mistral-7b-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:28:04.828687Z","iopub.execute_input":"2025-08-21T13:28:04.828990Z","iopub.status.idle":"2025-08-21T13:28:18.174296Z","shell.execute_reply.started":"2025-08-21T13:28:04.828968Z","shell.execute_reply":"2025-08-21T13:28:18.173750Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.8.9: Fast Qwen2 patching. Transformers: 4.55.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27feb48954dc4cbda37f4294c24a499e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68971caa3cf949168cc124cefde88276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372d112b1c744f4b88467322fde822ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf18f39320e40c89c019b02042b0c5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98058d8c0b224734ba905b974f537e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/107 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de4af8b4afb4107a4114b8adb37b8d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79976b62c10d4d7e84b8ab0d84f5e780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd1d6ed5e66486997578d1bfe71f3c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/37.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd6e9676ee44232ae5ca04e6f30d65e"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.8.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"run(k_shot=5, limit_per_subject=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:28:22.156947Z","iopub.execute_input":"2025-08-21T13:28:22.157427Z","iopub.status.idle":"2025-08-21T13:36:41.980593Z","shell.execute_reply.started":"2025-08-21T13:28:22.157404Z","shell.execute_reply":"2025-08-21T13:36:41.979863Z"}},"outputs":[{"name":"stdout","text":"\n=== Evaluating MedMCQA (with subject_name breakdown) ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b31177055c741cbb644f2bd168a1469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/85.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4245f259d09a4055bdb486f44ccb1d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/936k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f99eaf3a2a6b4887aeba1dd991f3e253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/1.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737a1933d36c4db8ab54eb9726c41f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7113c155f0c4adb989b4a7959ef422a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c00cce6dd2548e0b8bcc93f2dcf352e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af1288277a2491080673fa385863c91"}},"metadata":{}},{"name":"stdout","text":"===== SUBJECT: Anatomy =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:28<00:00,  3.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Anatomy: n = 100.0000, acc=0.3600\n===== SUBJECT: Biochemistry =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:20<00:00,  4.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Biochemistry: n = 100.0000, acc=0.4600\n===== SUBJECT: Surgery =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Surgery: n = 100.0000, acc=0.4900\n===== SUBJECT: Ophthalmology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ophthalmology: n = 100.0000, acc=0.3800\n===== SUBJECT: Physiology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Physiology: n = 100.0000, acc=0.4400\n===== SUBJECT: Social & Preventive Medicine =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Social & Preventive Medicine: n = 100.0000, acc=0.4700\n===== SUBJECT: Gynaecology & Obstetrics =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:24<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Gynaecology & Obstetrics: n = 100.0000, acc=0.4200\n===== SUBJECT: Anaesthesia =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Anaesthesia: n = 100.0000, acc=0.4100\n===== SUBJECT: Psychiatry =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Psychiatry: n = 100.0000, acc=0.5000\n===== SUBJECT: Microbiology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Microbiology: n = 100.0000, acc=0.4200\n===== SUBJECT: Medicine =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:20<00:00,  4.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Medicine: n = 100.0000, acc=0.4400\n===== SUBJECT: Pharmacology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pharmacology: n = 100.0000, acc=0.4400\n===== SUBJECT: Dental =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:28<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dental: n = 100.0000, acc=0.3200\n===== SUBJECT: ENT =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"ENT: n = 100.0000, acc=0.3800\n===== SUBJECT: Forensic Medicine =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Forensic Medicine: n = 100.0000, acc=0.3800\n===== SUBJECT: Pediatrics =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:22<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pediatrics: n = 100.0000, acc=0.4100\n===== SUBJECT: Orthopaedics =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:24<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Orthopaedics: n = 100.0000, acc=0.3500\n===== SUBJECT: Radiology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:20<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Radiology: n = 100.0000, acc=0.3300\n===== SUBJECT: Pathology =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:21<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pathology: n = 100.0000, acc=0.4700\n===== SUBJECT: Skin =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:20<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Skin: n = 100.0000, acc=0.4300\n===== SUBJECT: Unknown =====\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:22<00:00,  4.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unknown: n = 100.0000, acc=0.3900\n\n===== SUMMARY (MedMCQA) =====\nAnatomy                         acc=0.3600  n=100\nBiochemistry                    acc=0.4600  n=100\nSurgery                         acc=0.4900  n=100\nOphthalmology                   acc=0.3800  n=100\nPhysiology                      acc=0.4400  n=100\nSocial & Preventive Medicine    acc=0.4700  n=100\nGynaecology & Obstetrics        acc=0.4200  n=100\nAnaesthesia                     acc=0.4100  n=100\nPsychiatry                      acc=0.5000  n=100\nMicrobiology                    acc=0.4200  n=100\nMedicine                        acc=0.4400  n=100\nPharmacology                    acc=0.4400  n=100\nDental                          acc=0.3200  n=100\nENT                             acc=0.3800  n=100\nForensic Medicine               acc=0.3800  n=100\nPediatrics                      acc=0.4100  n=100\nOrthopaedics                    acc=0.3500  n=100\nRadiology                       acc=0.3300  n=100\nPathology                       acc=0.4700  n=100\nSkin                            acc=0.4300  n=100\nUnknown                         acc=0.3900  n=100\n_TOTAL_                         acc=0.4138  n=2100\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[EvalResult(subject='Anatomy', n=100, correct=36, accuracy=0.36),\n EvalResult(subject='Biochemistry', n=100, correct=46, accuracy=0.46),\n EvalResult(subject='Surgery', n=100, correct=49, accuracy=0.49),\n EvalResult(subject='Ophthalmology', n=100, correct=38, accuracy=0.38),\n EvalResult(subject='Physiology', n=100, correct=44, accuracy=0.44),\n EvalResult(subject='Social & Preventive Medicine', n=100, correct=47, accuracy=0.47),\n EvalResult(subject='Gynaecology & Obstetrics', n=100, correct=42, accuracy=0.42),\n EvalResult(subject='Anaesthesia', n=100, correct=41, accuracy=0.41),\n EvalResult(subject='Psychiatry', n=100, correct=50, accuracy=0.5),\n EvalResult(subject='Microbiology', n=100, correct=42, accuracy=0.42),\n EvalResult(subject='Medicine', n=100, correct=44, accuracy=0.44),\n EvalResult(subject='Pharmacology', n=100, correct=44, accuracy=0.44),\n EvalResult(subject='Dental', n=100, correct=32, accuracy=0.32),\n EvalResult(subject='ENT', n=100, correct=38, accuracy=0.38),\n EvalResult(subject='Forensic Medicine', n=100, correct=38, accuracy=0.38),\n EvalResult(subject='Pediatrics', n=100, correct=41, accuracy=0.41),\n EvalResult(subject='Orthopaedics', n=100, correct=35, accuracy=0.35),\n EvalResult(subject='Radiology', n=100, correct=33, accuracy=0.33),\n EvalResult(subject='Pathology', n=100, correct=47, accuracy=0.47),\n EvalResult(subject='Skin', n=100, correct=43, accuracy=0.43),\n EvalResult(subject='Unknown', n=100, correct=39, accuracy=0.39),\n EvalResult(subject='_TOTAL_', n=2100, correct=869, accuracy=0.4138095238095238)]"},"metadata":{}}],"execution_count":4}]}